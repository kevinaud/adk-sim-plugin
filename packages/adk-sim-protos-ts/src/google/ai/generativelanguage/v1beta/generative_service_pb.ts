// Copyright 2025 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.10.2 with parameter "target=ts"
// @generated from file google/ai/generativelanguage/v1beta/generative_service.proto (package google.ai.generativelanguage.v1beta, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from '@bufbuild/protobuf/codegenv2';
import { enumDesc, fileDesc, messageDesc, serviceDesc } from '@bufbuild/protobuf/codegenv2';
import type { CitationMetadata } from './citation_pb';
import { file_google_ai_generativelanguage_v1beta_citation } from './citation_pb';
import type {
  Blob,
  Content,
  FunctionCall,
  FunctionResponse,
  GroundingPassages,
  ModalityTokenCount,
  Schema,
  Tool,
  ToolConfig,
} from './content_pb';
import { file_google_ai_generativelanguage_v1beta_content } from './content_pb';
import type { MetadataFilter } from './retriever_pb';
import { file_google_ai_generativelanguage_v1beta_retriever } from './retriever_pb';
import type { SafetyRating, SafetySetting } from './safety_pb';
import { file_google_ai_generativelanguage_v1beta_safety } from './safety_pb';
import { file_google_api_annotations } from '../../../api/annotations_pb';
import { file_google_api_client } from '../../../api/client_pb';
import { file_google_api_field_behavior } from '../../../api/field_behavior_pb';
import { file_google_api_resource } from '../../../api/resource_pb';
import type { Duration, Value } from '@bufbuild/protobuf/wkt';
import { file_google_protobuf_duration, file_google_protobuf_struct } from '@bufbuild/protobuf/wkt';
import type { Message } from '@bufbuild/protobuf';

/**
 * Describes the file google/ai/generativelanguage/v1beta/generative_service.proto.
 */
export const file_google_ai_generativelanguage_v1beta_generative_service: GenFile =
  /*@__PURE__*/
  fileDesc(
    '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',
    [
      file_google_ai_generativelanguage_v1beta_citation,
      file_google_ai_generativelanguage_v1beta_content,
      file_google_ai_generativelanguage_v1beta_retriever,
      file_google_ai_generativelanguage_v1beta_safety,
      file_google_api_annotations,
      file_google_api_client,
      file_google_api_field_behavior,
      file_google_api_resource,
      file_google_protobuf_duration,
      file_google_protobuf_struct,
    ],
  );

/**
 * Request to generate a completion from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateContentRequest
 */
export type GenerateContentRequest =
  Message<'google.ai.generativelanguage.v1beta.GenerateContentRequest'> & {
    /**
     * Required. The name of the `Model` to use for generating the completion.
     *
     * Format: `models/{model}`.
     *
     * @generated from field: string model = 1;
     */
    model: string;

    /**
     * Optional. Developer set [system
     * instruction(s)](https://ai.google.dev/gemini-api/docs/system-instructions).
     * Currently, text only.
     *
     * @generated from field: optional google.ai.generativelanguage.v1beta.Content system_instruction = 8;
     */
    systemInstruction?: Content;

    /**
     * Required. The content of the current conversation with the model.
     *
     * For single-turn queries, this is a single instance. For multi-turn queries
     * like [chat](https://ai.google.dev/gemini-api/docs/text-generation#chat),
     * this is a repeated field that contains the conversation history and the
     * latest request.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.Content contents = 2;
     */
    contents: Content[];

    /**
     * Optional. A list of `Tools` the `Model` may use to generate the next
     * response.
     *
     * A `Tool` is a piece of code that enables the system to interact with
     * external systems to perform an action, or set of actions, outside of
     * knowledge and scope of the `Model`. Supported `Tool`s are `Function` and
     * `code_execution`. Refer to the [Function
     * calling](https://ai.google.dev/gemini-api/docs/function-calling) and the
     * [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)
     * guides to learn more.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.Tool tools = 5;
     */
    tools: Tool[];

    /**
     * Optional. Tool configuration for any `Tool` specified in the request. Refer
     * to the [Function calling
     * guide](https://ai.google.dev/gemini-api/docs/function-calling#function_calling_mode)
     * for a usage example.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.ToolConfig tool_config = 7;
     */
    toolConfig?: ToolConfig;

    /**
     * Optional. A list of unique `SafetySetting` instances for blocking unsafe
     * content.
     *
     * This will be enforced on the `GenerateContentRequest.contents` and
     * `GenerateContentResponse.candidates`. There should not be more than one
     * setting for each `SafetyCategory` type. The API will block any contents and
     * responses that fail to meet the thresholds set by these settings. This list
     * overrides the default settings for each `SafetyCategory` specified in the
     * safety_settings. If there is no `SafetySetting` for a given
     * `SafetyCategory` provided in the list, the API will use the default safety
     * setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
     * HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
     * HARM_CATEGORY_HARASSMENT, HARM_CATEGORY_CIVIC_INTEGRITY are supported.
     * Refer to the [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
     * for detailed information on available safety settings. Also refer to the
     * [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
     * learn how to incorporate safety considerations in your AI applications.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetySetting safety_settings = 3;
     */
    safetySettings: SafetySetting[];

    /**
     * Optional. Configuration options for model generation and outputs.
     *
     * @generated from field: optional google.ai.generativelanguage.v1beta.GenerationConfig generation_config = 4;
     */
    generationConfig?: GenerationConfig;

    /**
     * Optional. The name of the content
     * [cached](https://ai.google.dev/gemini-api/docs/caching) to use as context
     * to serve the prediction. Format: `cachedContents/{cachedContent}`
     *
     * @generated from field: optional string cached_content = 9;
     */
    cachedContent?: string;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateContentRequest.
 * Use `create(GenerateContentRequestSchema)` to create a new message.
 */
export const GenerateContentRequestSchema: GenMessage<GenerateContentRequest> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 0);

/**
 * The configuration for the prebuilt speaker to use.
 *
 * @generated from message google.ai.generativelanguage.v1beta.PrebuiltVoiceConfig
 */
export type PrebuiltVoiceConfig =
  Message<'google.ai.generativelanguage.v1beta.PrebuiltVoiceConfig'> & {
    /**
     * The name of the preset voice to use.
     *
     * @generated from field: optional string voice_name = 1;
     */
    voiceName?: string;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.PrebuiltVoiceConfig.
 * Use `create(PrebuiltVoiceConfigSchema)` to create a new message.
 */
export const PrebuiltVoiceConfigSchema: GenMessage<PrebuiltVoiceConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 1);

/**
 * The configuration for the voice to use.
 *
 * @generated from message google.ai.generativelanguage.v1beta.VoiceConfig
 */
export type VoiceConfig = Message<'google.ai.generativelanguage.v1beta.VoiceConfig'> & {
  /**
   * The configuration for the speaker to use.
   *
   * @generated from oneof google.ai.generativelanguage.v1beta.VoiceConfig.voice_config
   */
  voiceConfig:
    | {
        /**
         * The configuration for the prebuilt voice to use.
         *
         * @generated from field: google.ai.generativelanguage.v1beta.PrebuiltVoiceConfig prebuilt_voice_config = 1;
         */
        value: PrebuiltVoiceConfig;
        case: 'prebuiltVoiceConfig';
      }
    | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.VoiceConfig.
 * Use `create(VoiceConfigSchema)` to create a new message.
 */
export const VoiceConfigSchema: GenMessage<VoiceConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 2);

/**
 * The configuration for a single speaker in a multi speaker setup.
 *
 * @generated from message google.ai.generativelanguage.v1beta.SpeakerVoiceConfig
 */
export type SpeakerVoiceConfig =
  Message<'google.ai.generativelanguage.v1beta.SpeakerVoiceConfig'> & {
    /**
     * Required. The name of the speaker to use. Should be the same as in the
     * prompt.
     *
     * @generated from field: string speaker = 1;
     */
    speaker: string;

    /**
     * Required. The configuration for the voice to use.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.VoiceConfig voice_config = 2;
     */
    voiceConfig?: VoiceConfig;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.SpeakerVoiceConfig.
 * Use `create(SpeakerVoiceConfigSchema)` to create a new message.
 */
export const SpeakerVoiceConfigSchema: GenMessage<SpeakerVoiceConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 3);

/**
 * The configuration for the multi-speaker setup.
 *
 * @generated from message google.ai.generativelanguage.v1beta.MultiSpeakerVoiceConfig
 */
export type MultiSpeakerVoiceConfig =
  Message<'google.ai.generativelanguage.v1beta.MultiSpeakerVoiceConfig'> & {
    /**
     * Required. All the enabled speaker voices.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.SpeakerVoiceConfig speaker_voice_configs = 2;
     */
    speakerVoiceConfigs: SpeakerVoiceConfig[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.MultiSpeakerVoiceConfig.
 * Use `create(MultiSpeakerVoiceConfigSchema)` to create a new message.
 */
export const MultiSpeakerVoiceConfigSchema: GenMessage<MultiSpeakerVoiceConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 4);

/**
 * The speech generation config.
 *
 * @generated from message google.ai.generativelanguage.v1beta.SpeechConfig
 */
export type SpeechConfig = Message<'google.ai.generativelanguage.v1beta.SpeechConfig'> & {
  /**
   * The configuration in case of single-voice output.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.VoiceConfig voice_config = 1;
   */
  voiceConfig?: VoiceConfig;

  /**
   * Optional. The configuration for the multi-speaker setup.
   * It is mutually exclusive with the voice_config field.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.MultiSpeakerVoiceConfig multi_speaker_voice_config = 3;
   */
  multiSpeakerVoiceConfig?: MultiSpeakerVoiceConfig;

  /**
   * Optional. Language code (in BCP 47 format, e.g. "en-US") for speech
   * synthesis.
   *
   * Valid values are: de-DE, en-AU, en-GB, en-IN, en-US, es-US, fr-FR, hi-IN,
   * pt-BR, ar-XA, es-ES, fr-CA, id-ID, it-IT, ja-JP, tr-TR, vi-VN, bn-IN,
   * gu-IN, kn-IN, ml-IN, mr-IN, ta-IN, te-IN, nl-NL, ko-KR, cmn-CN, pl-PL,
   * ru-RU, and th-TH.
   *
   * @generated from field: string language_code = 2;
   */
  languageCode: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.SpeechConfig.
 * Use `create(SpeechConfigSchema)` to create a new message.
 */
export const SpeechConfigSchema: GenMessage<SpeechConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 5);

/**
 * Config for thinking features.
 *
 * @generated from message google.ai.generativelanguage.v1beta.ThinkingConfig
 */
export type ThinkingConfig = Message<'google.ai.generativelanguage.v1beta.ThinkingConfig'> & {
  /**
   * Indicates whether to include thoughts in the response.
   * If true, thoughts are returned only when available.
   *
   * @generated from field: optional bool include_thoughts = 1;
   */
  includeThoughts?: boolean;

  /**
   * The number of thoughts tokens that the model should generate.
   *
   * @generated from field: optional int32 thinking_budget = 2;
   */
  thinkingBudget?: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.ThinkingConfig.
 * Use `create(ThinkingConfigSchema)` to create a new message.
 */
export const ThinkingConfigSchema: GenMessage<ThinkingConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 6);

/**
 * Config for image generation features.
 *
 * @generated from message google.ai.generativelanguage.v1beta.ImageConfig
 */
export type ImageConfig = Message<'google.ai.generativelanguage.v1beta.ImageConfig'> & {
  /**
   * Optional. The aspect ratio of the image to generate. Supported aspect
   * ratios: 1:1, 2:3, 3:2, 3:4, 4:3, 9:16, 16:9, 21:9.
   *
   * If not specified, the model will choose a default aspect ratio based on any
   * reference images provided.
   *
   * @generated from field: optional string aspect_ratio = 1;
   */
  aspectRatio?: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.ImageConfig.
 * Use `create(ImageConfigSchema)` to create a new message.
 */
export const ImageConfigSchema: GenMessage<ImageConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 7);

/**
 * Configuration options for model generation and outputs. Not all parameters
 * are configurable for every model.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerationConfig
 */
export type GenerationConfig = Message<'google.ai.generativelanguage.v1beta.GenerationConfig'> & {
  /**
   * Optional. Number of generated responses to return. If unset, this will
   * default to 1. Please note that this doesn't work for previous generation
   * models (Gemini 1.0 family)
   *
   * @generated from field: optional int32 candidate_count = 1;
   */
  candidateCount?: number;

  /**
   * Optional. The set of character sequences (up to 5) that will stop output
   * generation. If specified, the API will stop at the first appearance of a
   * `stop_sequence`. The stop sequence will not be included as part of the
   * response.
   *
   * @generated from field: repeated string stop_sequences = 2;
   */
  stopSequences: string[];

  /**
   * Optional. The maximum number of tokens to include in a response candidate.
   *
   * Note: The default value varies by model, see the `Model.output_token_limit`
   * attribute of the `Model` returned from the `getModel` function.
   *
   * @generated from field: optional int32 max_output_tokens = 4;
   */
  maxOutputTokens?: number;

  /**
   * Optional. Controls the randomness of the output.
   *
   * Note: The default value varies by model, see the `Model.temperature`
   * attribute of the `Model` returned from the `getModel` function.
   *
   * Values can range from [0.0, 2.0].
   *
   * @generated from field: optional float temperature = 5;
   */
  temperature?: number;

  /**
   * Optional. The maximum cumulative probability of tokens to consider when
   * sampling.
   *
   * The model uses combined Top-k and Top-p (nucleus) sampling.
   *
   * Tokens are sorted based on their assigned probabilities so that only the
   * most likely tokens are considered. Top-k sampling directly limits the
   * maximum number of tokens to consider, while Nucleus sampling limits the
   * number of tokens based on the cumulative probability.
   *
   * Note: The default value varies by `Model` and is specified by
   * the`Model.top_p` attribute returned from the `getModel` function. An empty
   * `top_k` attribute indicates that the model doesn't apply top-k sampling
   * and doesn't allow setting `top_k` on requests.
   *
   * @generated from field: optional float top_p = 6;
   */
  topP?: number;

  /**
   * Optional. The maximum number of tokens to consider when sampling.
   *
   * Gemini models use Top-p (nucleus) sampling or a combination of Top-k and
   * nucleus sampling. Top-k sampling considers the set of `top_k` most probable
   * tokens. Models running with nucleus sampling don't allow top_k setting.
   *
   * Note: The default value varies by `Model` and is specified by
   * the`Model.top_p` attribute returned from the `getModel` function. An empty
   * `top_k` attribute indicates that the model doesn't apply top-k sampling
   * and doesn't allow setting `top_k` on requests.
   *
   * @generated from field: optional int32 top_k = 7;
   */
  topK?: number;

  /**
   * Optional. Seed used in decoding. If not set, the request uses a randomly
   * generated seed.
   *
   * @generated from field: optional int32 seed = 8;
   */
  seed?: number;

  /**
   * Optional. MIME type of the generated candidate text.
   * Supported MIME types are:
   * `text/plain`: (default) Text output.
   * `application/json`: JSON response in the response candidates.
   * `text/x.enum`: ENUM as a string response in the response candidates.
   * Refer to the
   * [docs](https://ai.google.dev/gemini-api/docs/prompting_with_media#plain_text_formats)
   * for a list of all supported text MIME types.
   *
   * @generated from field: string response_mime_type = 13;
   */
  responseMimeType: string;

  /**
   * Optional. Output schema of the generated candidate text. Schemas must be a
   * subset of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema)
   * and can be objects, primitives or arrays.
   *
   * If set, a compatible `response_mime_type` must also be set.
   * Compatible MIME types:
   * `application/json`: Schema for JSON response.
   * Refer to the [JSON text generation
   * guide](https://ai.google.dev/gemini-api/docs/json-mode) for more details.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.Schema response_schema = 14;
   */
  responseSchema?: Schema;

  /**
   * Optional. Output schema of the generated response. This is an alternative
   * to `response_schema` that accepts [JSON Schema](https://json-schema.org/).
   *
   * If set, `response_schema` must be omitted, but `response_mime_type` is
   * required.
   *
   * While the full JSON Schema may be sent, not all features are supported.
   * Specifically, only the following properties are supported:
   *
   * - `$id`
   * - `$defs`
   * - `$ref`
   * - `$anchor`
   * - `type`
   * - `format`
   * - `title`
   * - `description`
   * - `enum` (for strings and numbers)
   * - `items`
   * - `prefixItems`
   * - `minItems`
   * - `maxItems`
   * - `minimum`
   * - `maximum`
   * - `anyOf`
   * - `oneOf` (interpreted the same as `anyOf`)
   * - `properties`
   * - `additionalProperties`
   * - `required`
   *
   * The non-standard `propertyOrdering` property may also be set.
   *
   * Cyclic references are unrolled to a limited degree and, as such, may only
   * be used within non-required properties. (Nullable properties are not
   * sufficient.) If `$ref` is set on a sub-schema, no other properties, except
   * for than those starting as a `$`, may be set.
   *
   * @generated from field: google.protobuf.Value response_json_schema = 24 [json_name = "_responseJsonSchema"];
   */
  responseJsonSchema?: Value;

  /**
   * Optional. An internal detail. Use `responseJsonSchema` rather than this
   * field.
   *
   * @generated from field: google.protobuf.Value response_json_schema_ordered = 28 [json_name = "responseJsonSchema"];
   */
  responseJsonSchemaOrdered?: Value;

  /**
   * Optional. Presence penalty applied to the next token's logprobs if the
   * token has already been seen in the response.
   *
   * This penalty is binary on/off and not dependant on the number of times the
   * token is used (after the first). Use
   * [frequency_penalty][google.ai.generativelanguage.v1beta.GenerationConfig.frequency_penalty]
   * for a penalty that increases with each use.
   *
   * A positive penalty will discourage the use of tokens that have already
   * been used in the response, increasing the vocabulary.
   *
   * A negative penalty will encourage the use of tokens that have already been
   * used in the response, decreasing the vocabulary.
   *
   * @generated from field: optional float presence_penalty = 15;
   */
  presencePenalty?: number;

  /**
   * Optional. Frequency penalty applied to the next token's logprobs,
   * multiplied by the number of times each token has been seen in the respponse
   * so far.
   *
   * A positive penalty will discourage the use of tokens that have already
   * been used, proportional to the number of times the token has been used:
   * The more a token is used, the more difficult it is for the model to use
   * that token again increasing the vocabulary of responses.
   *
   * Caution: A _negative_ penalty will encourage the model to reuse tokens
   * proportional to the number of times the token has been used. Small
   * negative values will reduce the vocabulary of a response. Larger negative
   * values will cause the model to start repeating a common token  until it
   * hits the
   * [max_output_tokens][google.ai.generativelanguage.v1beta.GenerationConfig.max_output_tokens]
   * limit.
   *
   * @generated from field: optional float frequency_penalty = 16;
   */
  frequencyPenalty?: number;

  /**
   * Optional. If true, export the logprobs results in response.
   *
   * @generated from field: optional bool response_logprobs = 17;
   */
  responseLogprobs?: boolean;

  /**
   * Optional. Only valid if
   * [response_logprobs=True][google.ai.generativelanguage.v1beta.GenerationConfig.response_logprobs].
   * This sets the number of top logprobs to return at each decoding step in the
   * [Candidate.logprobs_result][google.ai.generativelanguage.v1beta.Candidate.logprobs_result].
   * The number must be in the range of [0, 20].
   *
   * @generated from field: optional int32 logprobs = 18;
   */
  logprobs?: number;

  /**
   * Optional. Enables enhanced civic answers. It may not be available for all
   * models.
   *
   * @generated from field: optional bool enable_enhanced_civic_answers = 19;
   */
  enableEnhancedCivicAnswers?: boolean;

  /**
   * Optional. The requested modalities of the response. Represents the set of
   * modalities that the model can return, and should be expected in the
   * response. This is an exact match to the modalities of the response.
   *
   * A model may have multiple combinations of supported modalities. If the
   * requested modalities do not match any of the supported combinations, an
   * error will be returned.
   *
   * An empty list is equivalent to requesting only text.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.GenerationConfig.Modality response_modalities = 20;
   */
  responseModalities: GenerationConfig_Modality[];

  /**
   * Optional. The speech generation config.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.SpeechConfig speech_config = 21;
   */
  speechConfig?: SpeechConfig;

  /**
   * Optional. Config for thinking features.
   * An error will be returned if this field is set for models that don't
   * support thinking.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.ThinkingConfig thinking_config = 22;
   */
  thinkingConfig?: ThinkingConfig;

  /**
   * Optional. Config for image generation.
   * An error will be returned if this field is set for models that don't
   * support these config options.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.ImageConfig image_config = 27;
   */
  imageConfig?: ImageConfig;

  /**
   * Optional. If specified, the media resolution specified will be used.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.GenerationConfig.MediaResolution media_resolution = 23;
   */
  mediaResolution?: GenerationConfig_MediaResolution;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerationConfig.
 * Use `create(GenerationConfigSchema)` to create a new message.
 */
export const GenerationConfigSchema: GenMessage<GenerationConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 8);

/**
 * Supported modalities of the response.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.GenerationConfig.Modality
 */
export enum GenerationConfig_Modality {
  /**
   * Default value.
   *
   * @generated from enum value: MODALITY_UNSPECIFIED = 0;
   */
  MODALITY_UNSPECIFIED = 0,

  /**
   * Indicates the model should return text.
   *
   * @generated from enum value: TEXT = 1;
   */
  TEXT = 1,

  /**
   * Indicates the model should return images.
   *
   * @generated from enum value: IMAGE = 2;
   */
  IMAGE = 2,

  /**
   * Indicates the model should return audio.
   *
   * @generated from enum value: AUDIO = 3;
   */
  AUDIO = 3,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.GenerationConfig.Modality.
 */
export const GenerationConfig_ModalitySchema: GenEnum<GenerationConfig_Modality> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 8, 0);

/**
 * Media resolution for the input media.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.GenerationConfig.MediaResolution
 */
export enum GenerationConfig_MediaResolution {
  /**
   * Media resolution has not been set.
   *
   * @generated from enum value: MEDIA_RESOLUTION_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * Media resolution set to low (64 tokens).
   *
   * @generated from enum value: MEDIA_RESOLUTION_LOW = 1;
   */
  LOW = 1,

  /**
   * Media resolution set to medium (256 tokens).
   *
   * @generated from enum value: MEDIA_RESOLUTION_MEDIUM = 2;
   */
  MEDIUM = 2,

  /**
   * Media resolution set to high (zoomed reframing with 256 tokens).
   *
   * @generated from enum value: MEDIA_RESOLUTION_HIGH = 3;
   */
  HIGH = 3,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.GenerationConfig.MediaResolution.
 */
export const GenerationConfig_MediaResolutionSchema: GenEnum<GenerationConfig_MediaResolution> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 8, 1);

/**
 * Configuration for retrieving grounding content from a `Corpus` or
 * `Document` created using the Semantic Retriever API.
 *
 * @generated from message google.ai.generativelanguage.v1beta.SemanticRetrieverConfig
 */
export type SemanticRetrieverConfig =
  Message<'google.ai.generativelanguage.v1beta.SemanticRetrieverConfig'> & {
    /**
     * Required. Name of the resource for retrieval. Example: `corpora/123` or
     * `corpora/123/documents/abc`.
     *
     * @generated from field: string source = 1;
     */
    source: string;

    /**
     * Required. Query to use for matching `Chunk`s in the given resource by
     * similarity.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.Content query = 2;
     */
    query?: Content;

    /**
     * Optional. Filters for selecting `Document`s and/or `Chunk`s from the
     * resource.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.MetadataFilter metadata_filters = 3;
     */
    metadataFilters: MetadataFilter[];

    /**
     * Optional. Maximum number of relevant `Chunk`s to retrieve.
     *
     * @generated from field: optional int32 max_chunks_count = 4;
     */
    maxChunksCount?: number;

    /**
     * Optional. Minimum relevance score for retrieved relevant `Chunk`s.
     *
     * @generated from field: optional float minimum_relevance_score = 5;
     */
    minimumRelevanceScore?: number;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.SemanticRetrieverConfig.
 * Use `create(SemanticRetrieverConfigSchema)` to create a new message.
 */
export const SemanticRetrieverConfigSchema: GenMessage<SemanticRetrieverConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 9);

/**
 * Response from the model supporting multiple candidate responses.
 *
 * Safety ratings and content filtering are reported for both
 * prompt in `GenerateContentResponse.prompt_feedback` and for each candidate
 * in `finish_reason` and in `safety_ratings`. The API:
 *  - Returns either all requested candidates or none of them
 *  - Returns no candidates at all only if there was something wrong with the
 *    prompt (check `prompt_feedback`)
 *  - Reports feedback on each candidate in `finish_reason` and
 *    `safety_ratings`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateContentResponse
 */
export type GenerateContentResponse =
  Message<'google.ai.generativelanguage.v1beta.GenerateContentResponse'> & {
    /**
     * Candidate responses from the model.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.Candidate candidates = 1;
     */
    candidates: Candidate[];

    /**
     * Returns the prompt's feedback related to the content filters.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback prompt_feedback = 2;
     */
    promptFeedback?: GenerateContentResponse_PromptFeedback;

    /**
     * Output only. Metadata on the generation requests' token usage.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.GenerateContentResponse.UsageMetadata usage_metadata = 3;
     */
    usageMetadata?: GenerateContentResponse_UsageMetadata;

    /**
     * Output only. The model version used to generate the response.
     *
     * @generated from field: string model_version = 4;
     */
    modelVersion: string;

    /**
     * Output only. response_id is used to identify each response.
     *
     * @generated from field: string response_id = 5;
     */
    responseId: string;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateContentResponse.
 * Use `create(GenerateContentResponseSchema)` to create a new message.
 */
export const GenerateContentResponseSchema: GenMessage<GenerateContentResponse> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 10);

/**
 * A set of the feedback metadata the prompt specified in
 * `GenerateContentRequest.content`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback
 */
export type GenerateContentResponse_PromptFeedback =
  Message<'google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback'> & {
    /**
     * Optional. If set, the prompt was blocked and no candidates are returned.
     * Rephrase the prompt.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback.BlockReason block_reason = 1;
     */
    blockReason: GenerateContentResponse_PromptFeedback_BlockReason;

    /**
     * Ratings for safety of the prompt.
     * There is at most one rating per category.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetyRating safety_ratings = 2;
     */
    safetyRatings: SafetyRating[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback.
 * Use `create(GenerateContentResponse_PromptFeedbackSchema)` to create a new message.
 */
export const GenerateContentResponse_PromptFeedbackSchema: GenMessage<GenerateContentResponse_PromptFeedback> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 10, 0);

/**
 * Specifies the reason why the prompt was blocked.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback.BlockReason
 */
export enum GenerateContentResponse_PromptFeedback_BlockReason {
  /**
   * Default value. This value is unused.
   *
   * @generated from enum value: BLOCK_REASON_UNSPECIFIED = 0;
   */
  BLOCK_REASON_UNSPECIFIED = 0,

  /**
   * Prompt was blocked due to safety reasons. Inspect `safety_ratings`
   * to understand which safety category blocked it.
   *
   * @generated from enum value: SAFETY = 1;
   */
  SAFETY = 1,

  /**
   * Prompt was blocked due to unknown reasons.
   *
   * @generated from enum value: OTHER = 2;
   */
  OTHER = 2,

  /**
   * Prompt was blocked due to the terms which are included from the
   * terminology blocklist.
   *
   * @generated from enum value: BLOCKLIST = 3;
   */
  BLOCKLIST = 3,

  /**
   * Prompt was blocked due to prohibited content.
   *
   * @generated from enum value: PROHIBITED_CONTENT = 4;
   */
  PROHIBITED_CONTENT = 4,

  /**
   * Candidates blocked due to unsafe image generation content.
   *
   * @generated from enum value: IMAGE_SAFETY = 5;
   */
  IMAGE_SAFETY = 5,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback.BlockReason.
 */
export const GenerateContentResponse_PromptFeedback_BlockReasonSchema: GenEnum<GenerateContentResponse_PromptFeedback_BlockReason> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 10, 0, 0);

/**
 * Metadata on the generation request's token usage.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateContentResponse.UsageMetadata
 */
export type GenerateContentResponse_UsageMetadata =
  Message<'google.ai.generativelanguage.v1beta.GenerateContentResponse.UsageMetadata'> & {
    /**
     * Number of tokens in the prompt. When `cached_content` is set, this is
     * still the total effective prompt size meaning this includes the number of
     * tokens in the cached content.
     *
     * @generated from field: int32 prompt_token_count = 1;
     */
    promptTokenCount: number;

    /**
     * Number of tokens in the cached part of the prompt (the cached content)
     *
     * @generated from field: int32 cached_content_token_count = 4;
     */
    cachedContentTokenCount: number;

    /**
     * Total number of tokens across all the generated response candidates.
     *
     * @generated from field: int32 candidates_token_count = 2;
     */
    candidatesTokenCount: number;

    /**
     * Output only. Number of tokens present in tool-use prompt(s).
     *
     * @generated from field: int32 tool_use_prompt_token_count = 8;
     */
    toolUsePromptTokenCount: number;

    /**
     * Output only. Number of tokens of thoughts for thinking models.
     *
     * @generated from field: int32 thoughts_token_count = 10;
     */
    thoughtsTokenCount: number;

    /**
     * Total token count for the generation request (prompt + response
     * candidates).
     *
     * @generated from field: int32 total_token_count = 3;
     */
    totalTokenCount: number;

    /**
     * Output only. List of modalities that were processed in the request input.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.ModalityTokenCount prompt_tokens_details = 5;
     */
    promptTokensDetails: ModalityTokenCount[];

    /**
     * Output only. List of modalities of the cached content in the request
     * input.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.ModalityTokenCount cache_tokens_details = 6;
     */
    cacheTokensDetails: ModalityTokenCount[];

    /**
     * Output only. List of modalities that were returned in the response.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.ModalityTokenCount candidates_tokens_details = 7;
     */
    candidatesTokensDetails: ModalityTokenCount[];

    /**
     * Output only. List of modalities that were processed for tool-use request
     * inputs.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.ModalityTokenCount tool_use_prompt_tokens_details = 9;
     */
    toolUsePromptTokensDetails: ModalityTokenCount[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateContentResponse.UsageMetadata.
 * Use `create(GenerateContentResponse_UsageMetadataSchema)` to create a new message.
 */
export const GenerateContentResponse_UsageMetadataSchema: GenMessage<GenerateContentResponse_UsageMetadata> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 10, 1);

/**
 * A response candidate generated from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta.Candidate
 */
export type Candidate = Message<'google.ai.generativelanguage.v1beta.Candidate'> & {
  /**
   * Output only. Index of the candidate in the list of response candidates.
   *
   * @generated from field: optional int32 index = 3;
   */
  index?: number;

  /**
   * Output only. Generated content returned from the model.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.Content content = 1;
   */
  content?: Content;

  /**
   * Optional. Output only. The reason why the model stopped generating tokens.
   *
   * If empty, the model has not stopped generating tokens.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.Candidate.FinishReason finish_reason = 2;
   */
  finishReason: Candidate_FinishReason;

  /**
   * Optional. Output only. Details the reason why the model stopped generating
   * tokens. This is populated only when `finish_reason` is set.
   *
   * @generated from field: optional string finish_message = 4;
   */
  finishMessage?: string;

  /**
   * List of ratings for the safety of a response candidate.
   *
   * There is at most one rating per category.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetyRating safety_ratings = 5;
   */
  safetyRatings: SafetyRating[];

  /**
   * Output only. Citation information for model-generated candidate.
   *
   * This field may be populated with recitation information for any text
   * included in the `content`. These are passages that are "recited" from
   * copyrighted material in the foundational LLM's training data.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.CitationMetadata citation_metadata = 6;
   */
  citationMetadata?: CitationMetadata;

  /**
   * Output only. Token count for this candidate.
   *
   * @generated from field: int32 token_count = 7;
   */
  tokenCount: number;

  /**
   * Output only. Attribution information for sources that contributed to a
   * grounded answer.
   *
   * This field is populated for `GenerateAnswer` calls.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.GroundingAttribution grounding_attributions = 8;
   */
  groundingAttributions: GroundingAttribution[];

  /**
   * Output only. Grounding metadata for the candidate.
   *
   * This field is populated for `GenerateContent` calls.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.GroundingMetadata grounding_metadata = 9;
   */
  groundingMetadata?: GroundingMetadata;

  /**
   * Output only. Average log probability score of the candidate.
   *
   * @generated from field: double avg_logprobs = 10;
   */
  avgLogprobs: number;

  /**
   * Output only. Log-likelihood scores for the response tokens and top tokens
   *
   * @generated from field: google.ai.generativelanguage.v1beta.LogprobsResult logprobs_result = 11;
   */
  logprobsResult?: LogprobsResult;

  /**
   * Output only. Metadata related to url context retrieval tool.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.UrlContextMetadata url_context_metadata = 13;
   */
  urlContextMetadata?: UrlContextMetadata;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.Candidate.
 * Use `create(CandidateSchema)` to create a new message.
 */
export const CandidateSchema: GenMessage<Candidate> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 11);

/**
 * Defines the reason why the model stopped generating tokens.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.Candidate.FinishReason
 */
export enum Candidate_FinishReason {
  /**
   * Default value. This value is unused.
   *
   * @generated from enum value: FINISH_REASON_UNSPECIFIED = 0;
   */
  FINISH_REASON_UNSPECIFIED = 0,

  /**
   * Natural stop point of the model or provided stop sequence.
   *
   * @generated from enum value: STOP = 1;
   */
  STOP = 1,

  /**
   * The maximum number of tokens as specified in the request was reached.
   *
   * @generated from enum value: MAX_TOKENS = 2;
   */
  MAX_TOKENS = 2,

  /**
   * The response candidate content was flagged for safety reasons.
   *
   * @generated from enum value: SAFETY = 3;
   */
  SAFETY = 3,

  /**
   * The response candidate content was flagged for recitation reasons.
   *
   * @generated from enum value: RECITATION = 4;
   */
  RECITATION = 4,

  /**
   * The response candidate content was flagged for using an unsupported
   * language.
   *
   * @generated from enum value: LANGUAGE = 6;
   */
  LANGUAGE = 6,

  /**
   * Unknown reason.
   *
   * @generated from enum value: OTHER = 5;
   */
  OTHER = 5,

  /**
   * Token generation stopped because the content contains forbidden terms.
   *
   * @generated from enum value: BLOCKLIST = 7;
   */
  BLOCKLIST = 7,

  /**
   * Token generation stopped for potentially containing prohibited content.
   *
   * @generated from enum value: PROHIBITED_CONTENT = 8;
   */
  PROHIBITED_CONTENT = 8,

  /**
   * Token generation stopped because the content potentially contains
   * Sensitive Personally Identifiable Information (SPII).
   *
   * @generated from enum value: SPII = 9;
   */
  SPII = 9,

  /**
   * The function call generated by the model is invalid.
   *
   * @generated from enum value: MALFORMED_FUNCTION_CALL = 10;
   */
  MALFORMED_FUNCTION_CALL = 10,

  /**
   * Token generation stopped because generated images contain safety
   * violations.
   *
   * @generated from enum value: IMAGE_SAFETY = 11;
   */
  IMAGE_SAFETY = 11,

  /**
   * Image generation stopped because generated images has other prohibited
   * content.
   *
   * @generated from enum value: IMAGE_PROHIBITED_CONTENT = 14;
   */
  IMAGE_PROHIBITED_CONTENT = 14,

  /**
   * Image generation stopped because of other miscellaneous issue.
   *
   * @generated from enum value: IMAGE_OTHER = 15;
   */
  IMAGE_OTHER = 15,

  /**
   * The model was expected to generate an image, but none was generated.
   *
   * @generated from enum value: NO_IMAGE = 16;
   */
  NO_IMAGE = 16,

  /**
   * Image generation stopped due to recitation.
   *
   * @generated from enum value: IMAGE_RECITATION = 17;
   */
  IMAGE_RECITATION = 17,

  /**
   * Model generated a tool call but no tools were enabled in the request.
   *
   * @generated from enum value: UNEXPECTED_TOOL_CALL = 12;
   */
  UNEXPECTED_TOOL_CALL = 12,

  /**
   * Model called too many tools consecutively, thus the system exited
   * execution.
   *
   * @generated from enum value: TOO_MANY_TOOL_CALLS = 13;
   */
  TOO_MANY_TOOL_CALLS = 13,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.Candidate.FinishReason.
 */
export const Candidate_FinishReasonSchema: GenEnum<Candidate_FinishReason> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 11, 0);

/**
 * Metadata related to url context retrieval tool.
 *
 * @generated from message google.ai.generativelanguage.v1beta.UrlContextMetadata
 */
export type UrlContextMetadata =
  Message<'google.ai.generativelanguage.v1beta.UrlContextMetadata'> & {
    /**
     * List of url context.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.UrlMetadata url_metadata = 1;
     */
    urlMetadata: UrlMetadata[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.UrlContextMetadata.
 * Use `create(UrlContextMetadataSchema)` to create a new message.
 */
export const UrlContextMetadataSchema: GenMessage<UrlContextMetadata> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 12);

/**
 * Context of the a single url retrieval.
 *
 * @generated from message google.ai.generativelanguage.v1beta.UrlMetadata
 */
export type UrlMetadata = Message<'google.ai.generativelanguage.v1beta.UrlMetadata'> & {
  /**
   * Retrieved url by the tool.
   *
   * @generated from field: string retrieved_url = 1;
   */
  retrievedUrl: string;

  /**
   * Status of the url retrieval.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.UrlMetadata.UrlRetrievalStatus url_retrieval_status = 2;
   */
  urlRetrievalStatus: UrlMetadata_UrlRetrievalStatus;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.UrlMetadata.
 * Use `create(UrlMetadataSchema)` to create a new message.
 */
export const UrlMetadataSchema: GenMessage<UrlMetadata> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 13);

/**
 * Status of the url retrieval.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.UrlMetadata.UrlRetrievalStatus
 */
export enum UrlMetadata_UrlRetrievalStatus {
  /**
   * Default value. This value is unused.
   *
   * @generated from enum value: URL_RETRIEVAL_STATUS_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * Url retrieval is successful.
   *
   * @generated from enum value: URL_RETRIEVAL_STATUS_SUCCESS = 1;
   */
  SUCCESS = 1,

  /**
   * Url retrieval is failed due to error.
   *
   * @generated from enum value: URL_RETRIEVAL_STATUS_ERROR = 2;
   */
  ERROR = 2,

  /**
   * Url retrieval is failed because the content is behind paywall.
   *
   * @generated from enum value: URL_RETRIEVAL_STATUS_PAYWALL = 3;
   */
  PAYWALL = 3,

  /**
   * Url retrieval is failed because the content is unsafe.
   *
   * @generated from enum value: URL_RETRIEVAL_STATUS_UNSAFE = 4;
   */
  UNSAFE = 4,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.UrlMetadata.UrlRetrievalStatus.
 */
export const UrlMetadata_UrlRetrievalStatusSchema: GenEnum<UrlMetadata_UrlRetrievalStatus> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 13, 0);

/**
 * Logprobs Result
 *
 * @generated from message google.ai.generativelanguage.v1beta.LogprobsResult
 */
export type LogprobsResult = Message<'google.ai.generativelanguage.v1beta.LogprobsResult'> & {
  /**
   * Sum of log probabilities for all tokens.
   *
   * @generated from field: optional float log_probability_sum = 3;
   */
  logProbabilitySum?: number;

  /**
   * Length = total number of decoding steps.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.LogprobsResult.TopCandidates top_candidates = 1;
   */
  topCandidates: LogprobsResult_TopCandidates[];

  /**
   * Length = total number of decoding steps.
   * The chosen candidates may or may not be in top_candidates.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.LogprobsResult.Candidate chosen_candidates = 2;
   */
  chosenCandidates: LogprobsResult_Candidate[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.LogprobsResult.
 * Use `create(LogprobsResultSchema)` to create a new message.
 */
export const LogprobsResultSchema: GenMessage<LogprobsResult> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 14);

/**
 * Candidate for the logprobs token and score.
 *
 * @generated from message google.ai.generativelanguage.v1beta.LogprobsResult.Candidate
 */
export type LogprobsResult_Candidate =
  Message<'google.ai.generativelanguage.v1beta.LogprobsResult.Candidate'> & {
    /**
     * The candidates token string value.
     *
     * @generated from field: optional string token = 1;
     */
    token?: string;

    /**
     * The candidates token id value.
     *
     * @generated from field: optional int32 token_id = 3;
     */
    tokenId?: number;

    /**
     * The candidate's log probability.
     *
     * @generated from field: optional float log_probability = 2;
     */
    logProbability?: number;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.LogprobsResult.Candidate.
 * Use `create(LogprobsResult_CandidateSchema)` to create a new message.
 */
export const LogprobsResult_CandidateSchema: GenMessage<LogprobsResult_Candidate> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 14, 0);

/**
 * Candidates with top log probabilities at each decoding step.
 *
 * @generated from message google.ai.generativelanguage.v1beta.LogprobsResult.TopCandidates
 */
export type LogprobsResult_TopCandidates =
  Message<'google.ai.generativelanguage.v1beta.LogprobsResult.TopCandidates'> & {
    /**
     * Sorted by log probability in descending order.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.LogprobsResult.Candidate candidates = 1;
     */
    candidates: LogprobsResult_Candidate[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.LogprobsResult.TopCandidates.
 * Use `create(LogprobsResult_TopCandidatesSchema)` to create a new message.
 */
export const LogprobsResult_TopCandidatesSchema: GenMessage<LogprobsResult_TopCandidates> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 14, 1);

/**
 * Identifier for the source contributing to this attribution.
 *
 * @generated from message google.ai.generativelanguage.v1beta.AttributionSourceId
 */
export type AttributionSourceId =
  Message<'google.ai.generativelanguage.v1beta.AttributionSourceId'> & {
    /**
     * @generated from oneof google.ai.generativelanguage.v1beta.AttributionSourceId.source
     */
    source:
      | {
          /**
           * Identifier for an inline passage.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.AttributionSourceId.GroundingPassageId grounding_passage = 1;
           */
          value: AttributionSourceId_GroundingPassageId;
          case: 'groundingPassage';
        }
      | {
          /**
           * Identifier for a `Chunk` fetched via Semantic Retriever.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.AttributionSourceId.SemanticRetrieverChunk semantic_retriever_chunk = 2;
           */
          value: AttributionSourceId_SemanticRetrieverChunk;
          case: 'semanticRetrieverChunk';
        }
      | { case: undefined; value?: undefined };
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.AttributionSourceId.
 * Use `create(AttributionSourceIdSchema)` to create a new message.
 */
export const AttributionSourceIdSchema: GenMessage<AttributionSourceId> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 15);

/**
 * Identifier for a part within a `GroundingPassage`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.AttributionSourceId.GroundingPassageId
 */
export type AttributionSourceId_GroundingPassageId =
  Message<'google.ai.generativelanguage.v1beta.AttributionSourceId.GroundingPassageId'> & {
    /**
     * Output only. ID of the passage matching the `GenerateAnswerRequest`'s
     * `GroundingPassage.id`.
     *
     * @generated from field: string passage_id = 1;
     */
    passageId: string;

    /**
     * Output only. Index of the part within the `GenerateAnswerRequest`'s
     * `GroundingPassage.content`.
     *
     * @generated from field: int32 part_index = 2;
     */
    partIndex: number;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.AttributionSourceId.GroundingPassageId.
 * Use `create(AttributionSourceId_GroundingPassageIdSchema)` to create a new message.
 */
export const AttributionSourceId_GroundingPassageIdSchema: GenMessage<AttributionSourceId_GroundingPassageId> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 15, 0);

/**
 * Identifier for a `Chunk` retrieved via Semantic Retriever specified in the
 * `GenerateAnswerRequest` using `SemanticRetrieverConfig`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.AttributionSourceId.SemanticRetrieverChunk
 */
export type AttributionSourceId_SemanticRetrieverChunk =
  Message<'google.ai.generativelanguage.v1beta.AttributionSourceId.SemanticRetrieverChunk'> & {
    /**
     * Output only. Name of the source matching the request's
     * `SemanticRetrieverConfig.source`. Example: `corpora/123` or
     * `corpora/123/documents/abc`
     *
     * @generated from field: string source = 1;
     */
    source: string;

    /**
     * Output only. Name of the `Chunk` containing the attributed text.
     * Example: `corpora/123/documents/abc/chunks/xyz`
     *
     * @generated from field: string chunk = 2;
     */
    chunk: string;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.AttributionSourceId.SemanticRetrieverChunk.
 * Use `create(AttributionSourceId_SemanticRetrieverChunkSchema)` to create a new message.
 */
export const AttributionSourceId_SemanticRetrieverChunkSchema: GenMessage<AttributionSourceId_SemanticRetrieverChunk> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 15, 1);

/**
 * Attribution for a source that contributed to an answer.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingAttribution
 */
export type GroundingAttribution =
  Message<'google.ai.generativelanguage.v1beta.GroundingAttribution'> & {
    /**
     * Output only. Identifier for the source contributing to this attribution.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.AttributionSourceId source_id = 3;
     */
    sourceId?: AttributionSourceId;

    /**
     * Grounding source content that makes up this attribution.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.Content content = 2;
     */
    content?: Content;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingAttribution.
 * Use `create(GroundingAttributionSchema)` to create a new message.
 */
export const GroundingAttributionSchema: GenMessage<GroundingAttribution> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 16);

/**
 * Metadata related to retrieval in the grounding flow.
 *
 * @generated from message google.ai.generativelanguage.v1beta.RetrievalMetadata
 */
export type RetrievalMetadata = Message<'google.ai.generativelanguage.v1beta.RetrievalMetadata'> & {
  /**
   * Optional. Score indicating how likely information from google search could
   * help answer the prompt. The score is in the range [0, 1], where 0 is the
   * least likely and 1 is the most likely. This score is only populated when
   * google search grounding and dynamic retrieval is enabled. It will be
   * compared to the threshold to determine whether to trigger google search.
   *
   * @generated from field: float google_search_dynamic_retrieval_score = 2;
   */
  googleSearchDynamicRetrievalScore: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.RetrievalMetadata.
 * Use `create(RetrievalMetadataSchema)` to create a new message.
 */
export const RetrievalMetadataSchema: GenMessage<RetrievalMetadata> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 17);

/**
 * Metadata returned to client when grounding is enabled.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingMetadata
 */
export type GroundingMetadata = Message<'google.ai.generativelanguage.v1beta.GroundingMetadata'> & {
  /**
   * Optional. Google search entry for the following-up web searches.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.SearchEntryPoint search_entry_point = 1;
   */
  searchEntryPoint?: SearchEntryPoint;

  /**
   * List of supporting references retrieved from specified grounding source.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.GroundingChunk grounding_chunks = 2;
   */
  groundingChunks: GroundingChunk[];

  /**
   * List of grounding support.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.GroundingSupport grounding_supports = 3;
   */
  groundingSupports: GroundingSupport[];

  /**
   * Metadata related to retrieval in the grounding flow.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.RetrievalMetadata retrieval_metadata = 4;
   */
  retrievalMetadata?: RetrievalMetadata;

  /**
   * Web search queries for the following-up web search.
   *
   * @generated from field: repeated string web_search_queries = 5;
   */
  webSearchQueries: string[];

  /**
   * Optional. Resource name of the Google Maps widget context token that can be
   * used with the PlacesContextElement widget in order to render contextual
   * data. Only populated in the case that grounding with Google Maps is
   * enabled.
   *
   * @generated from field: optional string google_maps_widget_context_token = 7;
   */
  googleMapsWidgetContextToken?: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingMetadata.
 * Use `create(GroundingMetadataSchema)` to create a new message.
 */
export const GroundingMetadataSchema: GenMessage<GroundingMetadata> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 18);

/**
 * Google search entry point.
 *
 * @generated from message google.ai.generativelanguage.v1beta.SearchEntryPoint
 */
export type SearchEntryPoint = Message<'google.ai.generativelanguage.v1beta.SearchEntryPoint'> & {
  /**
   * Optional. Web content snippet that can be embedded in a web page or an app
   * webview.
   *
   * @generated from field: string rendered_content = 1;
   */
  renderedContent: string;

  /**
   * Optional. Base64 encoded JSON representing array of <search term, search
   * url> tuple.
   *
   * @generated from field: bytes sdk_blob = 2;
   */
  sdkBlob: Uint8Array;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.SearchEntryPoint.
 * Use `create(SearchEntryPointSchema)` to create a new message.
 */
export const SearchEntryPointSchema: GenMessage<SearchEntryPoint> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 19);

/**
 * Grounding chunk.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingChunk
 */
export type GroundingChunk = Message<'google.ai.generativelanguage.v1beta.GroundingChunk'> & {
  /**
   * Chunk type.
   *
   * @generated from oneof google.ai.generativelanguage.v1beta.GroundingChunk.chunk_type
   */
  chunkType:
    | {
        /**
         * Grounding chunk from the web.
         *
         * @generated from field: google.ai.generativelanguage.v1beta.GroundingChunk.Web web = 1;
         */
        value: GroundingChunk_Web;
        case: 'web';
      }
    | {
        /**
         * Optional. Grounding chunk from context retrieved by the file search tool.
         *
         * @generated from field: google.ai.generativelanguage.v1beta.GroundingChunk.RetrievedContext retrieved_context = 2;
         */
        value: GroundingChunk_RetrievedContext;
        case: 'retrievedContext';
      }
    | {
        /**
         * Optional. Grounding chunk from Google Maps.
         *
         * @generated from field: google.ai.generativelanguage.v1beta.GroundingChunk.Maps maps = 3;
         */
        value: GroundingChunk_Maps;
        case: 'maps';
      }
    | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingChunk.
 * Use `create(GroundingChunkSchema)` to create a new message.
 */
export const GroundingChunkSchema: GenMessage<GroundingChunk> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 20);

/**
 * Chunk from the web.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingChunk.Web
 */
export type GroundingChunk_Web =
  Message<'google.ai.generativelanguage.v1beta.GroundingChunk.Web'> & {
    /**
     * URI reference of the chunk.
     *
     * @generated from field: optional string uri = 1;
     */
    uri?: string;

    /**
     * Title of the chunk.
     *
     * @generated from field: optional string title = 2;
     */
    title?: string;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingChunk.Web.
 * Use `create(GroundingChunk_WebSchema)` to create a new message.
 */
export const GroundingChunk_WebSchema: GenMessage<GroundingChunk_Web> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 20, 0);

/**
 * Chunk from context retrieved by the file search tool.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingChunk.RetrievedContext
 */
export type GroundingChunk_RetrievedContext =
  Message<'google.ai.generativelanguage.v1beta.GroundingChunk.RetrievedContext'> & {
    /**
     * Optional. URI reference of the semantic retrieval document.
     *
     * @generated from field: optional string uri = 1;
     */
    uri?: string;

    /**
     * Optional. Title of the document.
     *
     * @generated from field: optional string title = 2;
     */
    title?: string;

    /**
     * Optional. Text of the chunk.
     *
     * @generated from field: optional string text = 3;
     */
    text?: string;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingChunk.RetrievedContext.
 * Use `create(GroundingChunk_RetrievedContextSchema)` to create a new message.
 */
export const GroundingChunk_RetrievedContextSchema: GenMessage<GroundingChunk_RetrievedContext> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 20, 1);

/**
 * A grounding chunk from Google Maps. A Maps chunk corresponds to a single
 * place.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingChunk.Maps
 */
export type GroundingChunk_Maps =
  Message<'google.ai.generativelanguage.v1beta.GroundingChunk.Maps'> & {
    /**
     * URI reference of the place.
     *
     * @generated from field: optional string uri = 1;
     */
    uri?: string;

    /**
     * Title of the place.
     *
     * @generated from field: optional string title = 2;
     */
    title?: string;

    /**
     * Text description of the place answer.
     *
     * @generated from field: optional string text = 3;
     */
    text?: string;

    /**
     * This ID of the place, in `places/{place_id}` format. A user can use this
     * ID to look up that place.
     *
     * @generated from field: optional string place_id = 4;
     */
    placeId?: string;

    /**
     * Sources that provide answers about the features of a given place in
     * Google Maps.
     *
     * @generated from field: optional google.ai.generativelanguage.v1beta.GroundingChunk.Maps.PlaceAnswerSources place_answer_sources = 5;
     */
    placeAnswerSources?: GroundingChunk_Maps_PlaceAnswerSources;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingChunk.Maps.
 * Use `create(GroundingChunk_MapsSchema)` to create a new message.
 */
export const GroundingChunk_MapsSchema: GenMessage<GroundingChunk_Maps> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 20, 2);

/**
 * Collection of sources that provide answers about the features of a given
 * place in Google Maps. Each PlaceAnswerSources message corresponds to a
 * specific place in Google Maps. The Google Maps tool used these sources in
 * order to answer questions about features of the place (e.g: "does Bar Foo
 * have Wifi" or "is Foo Bar wheelchair accessible?"). Currently we only
 * support review snippets as sources.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingChunk.Maps.PlaceAnswerSources
 */
export type GroundingChunk_Maps_PlaceAnswerSources =
  Message<'google.ai.generativelanguage.v1beta.GroundingChunk.Maps.PlaceAnswerSources'> & {
    /**
     * Snippets of reviews that are used to generate answers about the
     * features of a given place in Google Maps.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.GroundingChunk.Maps.PlaceAnswerSources.ReviewSnippet review_snippets = 1;
     */
    reviewSnippets: GroundingChunk_Maps_PlaceAnswerSources_ReviewSnippet[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingChunk.Maps.PlaceAnswerSources.
 * Use `create(GroundingChunk_Maps_PlaceAnswerSourcesSchema)` to create a new message.
 */
export const GroundingChunk_Maps_PlaceAnswerSourcesSchema: GenMessage<GroundingChunk_Maps_PlaceAnswerSources> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 20, 2, 0);

/**
 * Encapsulates a snippet of a user review that answers a question about
 * the features of a specific place in Google Maps.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingChunk.Maps.PlaceAnswerSources.ReviewSnippet
 */
export type GroundingChunk_Maps_PlaceAnswerSources_ReviewSnippet =
  Message<'google.ai.generativelanguage.v1beta.GroundingChunk.Maps.PlaceAnswerSources.ReviewSnippet'> & {
    /**
     * The ID of the review snippet.
     *
     * @generated from field: optional string review_id = 1;
     */
    reviewId?: string;

    /**
     * A link that corresponds to the user review on Google Maps.
     *
     * @generated from field: optional string google_maps_uri = 2;
     */
    googleMapsUri?: string;

    /**
     * Title of the review.
     *
     * @generated from field: optional string title = 3;
     */
    title?: string;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingChunk.Maps.PlaceAnswerSources.ReviewSnippet.
 * Use `create(GroundingChunk_Maps_PlaceAnswerSources_ReviewSnippetSchema)` to create a new message.
 */
export const GroundingChunk_Maps_PlaceAnswerSources_ReviewSnippetSchema: GenMessage<GroundingChunk_Maps_PlaceAnswerSources_ReviewSnippet> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 20, 2, 0, 0);

/**
 * Segment of the content.
 *
 * @generated from message google.ai.generativelanguage.v1beta.Segment
 */
export type Segment = Message<'google.ai.generativelanguage.v1beta.Segment'> & {
  /**
   * Output only. The index of a Part object within its parent Content object.
   *
   * @generated from field: int32 part_index = 1;
   */
  partIndex: number;

  /**
   * Output only. Start index in the given Part, measured in bytes. Offset from
   * the start of the Part, inclusive, starting at zero.
   *
   * @generated from field: int32 start_index = 2;
   */
  startIndex: number;

  /**
   * Output only. End index in the given Part, measured in bytes. Offset from
   * the start of the Part, exclusive, starting at zero.
   *
   * @generated from field: int32 end_index = 3;
   */
  endIndex: number;

  /**
   * Output only. The text corresponding to the segment from the response.
   *
   * @generated from field: string text = 4;
   */
  text: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.Segment.
 * Use `create(SegmentSchema)` to create a new message.
 */
export const SegmentSchema: GenMessage<Segment> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 21);

/**
 * Grounding support.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingSupport
 */
export type GroundingSupport = Message<'google.ai.generativelanguage.v1beta.GroundingSupport'> & {
  /**
   * Segment of the content this support belongs to.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.Segment segment = 1;
   */
  segment?: Segment;

  /**
   * A list of indices (into 'grounding_chunk') specifying the
   * citations associated with the claim. For instance [1,3,4] means
   * that grounding_chunk[1], grounding_chunk[3],
   * grounding_chunk[4] are the retrieved content attributed to the claim.
   *
   * @generated from field: repeated int32 grounding_chunk_indices = 2;
   */
  groundingChunkIndices: number[];

  /**
   * Confidence score of the support references. Ranges from 0 to 1. 1 is the
   * most confident. This list must have the same size as the
   * grounding_chunk_indices.
   *
   * @generated from field: repeated float confidence_scores = 3;
   */
  confidenceScores: number[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingSupport.
 * Use `create(GroundingSupportSchema)` to create a new message.
 */
export const GroundingSupportSchema: GenMessage<GroundingSupport> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 22);

/**
 * Request to generate a grounded answer from the `Model`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateAnswerRequest
 */
export type GenerateAnswerRequest =
  Message<'google.ai.generativelanguage.v1beta.GenerateAnswerRequest'> & {
    /**
     * The sources in which to ground the answer.
     *
     * @generated from oneof google.ai.generativelanguage.v1beta.GenerateAnswerRequest.grounding_source
     */
    groundingSource:
      | {
          /**
           * Passages provided inline with the request.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.GroundingPassages inline_passages = 6;
           */
          value: GroundingPassages;
          case: 'inlinePassages';
        }
      | {
          /**
           * Content retrieved from resources created via the Semantic Retriever
           * API.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.SemanticRetrieverConfig semantic_retriever = 7;
           */
          value: SemanticRetrieverConfig;
          case: 'semanticRetriever';
        }
      | { case: undefined; value?: undefined };

    /**
     * Required. The name of the `Model` to use for generating the grounded
     * response.
     *
     * Format: `model=models/{model}`.
     *
     * @generated from field: string model = 1;
     */
    model: string;

    /**
     * Required. The content of the current conversation with the `Model`. For
     * single-turn queries, this is a single question to answer. For multi-turn
     * queries, this is a repeated field that contains conversation history and
     * the last `Content` in the list containing the question.
     *
     * Note: `GenerateAnswer` only supports queries in English.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.Content contents = 2;
     */
    contents: Content[];

    /**
     * Required. Style in which answers should be returned.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.GenerateAnswerRequest.AnswerStyle answer_style = 5;
     */
    answerStyle: GenerateAnswerRequest_AnswerStyle;

    /**
     * Optional. A list of unique `SafetySetting` instances for blocking unsafe
     * content.
     *
     * This will be enforced on the `GenerateAnswerRequest.contents` and
     * `GenerateAnswerResponse.candidate`. There should not be more than one
     * setting for each `SafetyCategory` type. The API will block any contents and
     * responses that fail to meet the thresholds set by these settings. This list
     * overrides the default settings for each `SafetyCategory` specified in the
     * safety_settings. If there is no `SafetySetting` for a given
     * `SafetyCategory` provided in the list, the API will use the default safety
     * setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
     * HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
     * HARM_CATEGORY_HARASSMENT are supported.
     * Refer to the
     * [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
     * for detailed information on available safety settings. Also refer to the
     * [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
     * learn how to incorporate safety considerations in your AI applications.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetySetting safety_settings = 3;
     */
    safetySettings: SafetySetting[];

    /**
     * Optional. Controls the randomness of the output.
     *
     * Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will
     * produce responses that are more varied and creative, while a value closer
     * to 0.0 will typically result in more straightforward responses from the
     * model. A low temperature (~0.2) is usually recommended for
     * Attributed-Question-Answering use cases.
     *
     * @generated from field: optional float temperature = 4;
     */
    temperature?: number;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateAnswerRequest.
 * Use `create(GenerateAnswerRequestSchema)` to create a new message.
 */
export const GenerateAnswerRequestSchema: GenMessage<GenerateAnswerRequest> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 23);

/**
 * Style for grounded answers.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.GenerateAnswerRequest.AnswerStyle
 */
export enum GenerateAnswerRequest_AnswerStyle {
  /**
   * Unspecified answer style.
   *
   * @generated from enum value: ANSWER_STYLE_UNSPECIFIED = 0;
   */
  ANSWER_STYLE_UNSPECIFIED = 0,

  /**
   * Succinct but abstract style.
   *
   * @generated from enum value: ABSTRACTIVE = 1;
   */
  ABSTRACTIVE = 1,

  /**
   * Very brief and extractive style.
   *
   * @generated from enum value: EXTRACTIVE = 2;
   */
  EXTRACTIVE = 2,

  /**
   * Verbose style including extra details. The response may be formatted as a
   * sentence, paragraph, multiple paragraphs, or bullet points, etc.
   *
   * @generated from enum value: VERBOSE = 3;
   */
  VERBOSE = 3,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.GenerateAnswerRequest.AnswerStyle.
 */
export const GenerateAnswerRequest_AnswerStyleSchema: GenEnum<GenerateAnswerRequest_AnswerStyle> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 23, 0);

/**
 * Response from the model for a grounded answer.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateAnswerResponse
 */
export type GenerateAnswerResponse =
  Message<'google.ai.generativelanguage.v1beta.GenerateAnswerResponse'> & {
    /**
     * Candidate answer from the model.
     *
     * Note: The model *always* attempts to provide a grounded answer, even when
     * the answer is unlikely to be answerable from the given passages.
     * In that case, a low-quality or ungrounded answer may be provided, along
     * with a low `answerable_probability`.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.Candidate answer = 1;
     */
    answer?: Candidate;

    /**
     * Output only. The model's estimate of the probability that its answer is
     * correct and grounded in the input passages.
     *
     * A low `answerable_probability` indicates that the answer might not be
     * grounded in the sources.
     *
     * When `answerable_probability` is low, you may want to:
     *
     * * Display a message to the effect of "We couldnt answer that question" to
     * the user.
     * * Fall back to a general-purpose LLM that answers the question from world
     * knowledge. The threshold and nature of such fallbacks will depend on
     * individual use cases. `0.5` is a good starting threshold.
     *
     * @generated from field: optional float answerable_probability = 2;
     */
    answerableProbability?: number;

    /**
     * Output only. Feedback related to the input data used to answer the
     * question, as opposed to the model-generated response to the question.
     *
     * The input data can be one or more of the following:
     *
     * - Question specified by the last entry in `GenerateAnswerRequest.content`
     * - Conversation history specified by the other entries in
     * `GenerateAnswerRequest.content`
     * - Grounding sources (`GenerateAnswerRequest.semantic_retriever` or
     * `GenerateAnswerRequest.inline_passages`)
     *
     * @generated from field: optional google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback input_feedback = 3;
     */
    inputFeedback?: GenerateAnswerResponse_InputFeedback;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateAnswerResponse.
 * Use `create(GenerateAnswerResponseSchema)` to create a new message.
 */
export const GenerateAnswerResponseSchema: GenMessage<GenerateAnswerResponse> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 24);

/**
 * Feedback related to the input data used to answer the question, as opposed
 * to the model-generated response to the question.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback
 */
export type GenerateAnswerResponse_InputFeedback =
  Message<'google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback'> & {
    /**
     * Optional. If set, the input was blocked and no candidates are returned.
     * Rephrase the input.
     *
     * @generated from field: optional google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback.BlockReason block_reason = 1;
     */
    blockReason?: GenerateAnswerResponse_InputFeedback_BlockReason;

    /**
     * Ratings for safety of the input.
     * There is at most one rating per category.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetyRating safety_ratings = 2;
     */
    safetyRatings: SafetyRating[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback.
 * Use `create(GenerateAnswerResponse_InputFeedbackSchema)` to create a new message.
 */
export const GenerateAnswerResponse_InputFeedbackSchema: GenMessage<GenerateAnswerResponse_InputFeedback> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 24, 0);

/**
 * Specifies what was the reason why input was blocked.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback.BlockReason
 */
export enum GenerateAnswerResponse_InputFeedback_BlockReason {
  /**
   * Default value. This value is unused.
   *
   * @generated from enum value: BLOCK_REASON_UNSPECIFIED = 0;
   */
  BLOCK_REASON_UNSPECIFIED = 0,

  /**
   * Input was blocked due to safety reasons. Inspect
   * `safety_ratings` to understand which safety category blocked it.
   *
   * @generated from enum value: SAFETY = 1;
   */
  SAFETY = 1,

  /**
   * Input was blocked due to other reasons.
   *
   * @generated from enum value: OTHER = 2;
   */
  OTHER = 2,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback.BlockReason.
 */
export const GenerateAnswerResponse_InputFeedback_BlockReasonSchema: GenEnum<GenerateAnswerResponse_InputFeedback_BlockReason> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 24, 0, 0);

/**
 * Request containing the `Content` for the model to embed.
 *
 * @generated from message google.ai.generativelanguage.v1beta.EmbedContentRequest
 */
export type EmbedContentRequest =
  Message<'google.ai.generativelanguage.v1beta.EmbedContentRequest'> & {
    /**
     * Required. The model's resource name. This serves as an ID for the Model to
     * use.
     *
     * This name should match a model name returned by the `ListModels` method.
     *
     * Format: `models/{model}`
     *
     * @generated from field: string model = 1;
     */
    model: string;

    /**
     * Required. The content to embed. Only the `parts.text` fields will be
     * counted.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.Content content = 2;
     */
    content?: Content;

    /**
     * Optional. Optional task type for which the embeddings will be used. Not
     * supported on earlier models (`models/embedding-001`).
     *
     * @generated from field: optional google.ai.generativelanguage.v1beta.TaskType task_type = 3;
     */
    taskType?: TaskType;

    /**
     * Optional. An optional title for the text. Only applicable when TaskType is
     * `RETRIEVAL_DOCUMENT`.
     *
     * Note: Specifying a `title` for `RETRIEVAL_DOCUMENT` provides better quality
     * embeddings for retrieval.
     *
     * @generated from field: optional string title = 4;
     */
    title?: string;

    /**
     * Optional. Optional reduced dimension for the output embedding. If set,
     * excessive values in the output embedding are truncated from the end.
     * Supported by newer models since 2024 only. You cannot set this value if
     * using the earlier model (`models/embedding-001`).
     *
     * @generated from field: optional int32 output_dimensionality = 5;
     */
    outputDimensionality?: number;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.EmbedContentRequest.
 * Use `create(EmbedContentRequestSchema)` to create a new message.
 */
export const EmbedContentRequestSchema: GenMessage<EmbedContentRequest> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 25);

/**
 * A list of floats representing an embedding.
 *
 * @generated from message google.ai.generativelanguage.v1beta.ContentEmbedding
 */
export type ContentEmbedding = Message<'google.ai.generativelanguage.v1beta.ContentEmbedding'> & {
  /**
   * The embedding values.
   *
   * @generated from field: repeated float values = 1;
   */
  values: number[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.ContentEmbedding.
 * Use `create(ContentEmbeddingSchema)` to create a new message.
 */
export const ContentEmbeddingSchema: GenMessage<ContentEmbedding> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 26);

/**
 * The response to an `EmbedContentRequest`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.EmbedContentResponse
 */
export type EmbedContentResponse =
  Message<'google.ai.generativelanguage.v1beta.EmbedContentResponse'> & {
    /**
     * Output only. The embedding generated from the input content.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.ContentEmbedding embedding = 1;
     */
    embedding?: ContentEmbedding;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.EmbedContentResponse.
 * Use `create(EmbedContentResponseSchema)` to create a new message.
 */
export const EmbedContentResponseSchema: GenMessage<EmbedContentResponse> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 27);

/**
 * Batch request to get embeddings from the model for a list of prompts.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BatchEmbedContentsRequest
 */
export type BatchEmbedContentsRequest =
  Message<'google.ai.generativelanguage.v1beta.BatchEmbedContentsRequest'> & {
    /**
     * Required. The model's resource name. This serves as an ID for the Model to
     * use.
     *
     * This name should match a model name returned by the `ListModels` method.
     *
     * Format: `models/{model}`
     *
     * @generated from field: string model = 1;
     */
    model: string;

    /**
     * Required. Embed requests for the batch. The model in each of these requests
     * must match the model specified `BatchEmbedContentsRequest.model`.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.EmbedContentRequest requests = 2;
     */
    requests: EmbedContentRequest[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BatchEmbedContentsRequest.
 * Use `create(BatchEmbedContentsRequestSchema)` to create a new message.
 */
export const BatchEmbedContentsRequestSchema: GenMessage<BatchEmbedContentsRequest> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 28);

/**
 * The response to a `BatchEmbedContentsRequest`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BatchEmbedContentsResponse
 */
export type BatchEmbedContentsResponse =
  Message<'google.ai.generativelanguage.v1beta.BatchEmbedContentsResponse'> & {
    /**
     * Output only. The embeddings for each request, in the same order as provided
     * in the batch request.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.ContentEmbedding embeddings = 1;
     */
    embeddings: ContentEmbedding[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BatchEmbedContentsResponse.
 * Use `create(BatchEmbedContentsResponseSchema)` to create a new message.
 */
export const BatchEmbedContentsResponseSchema: GenMessage<BatchEmbedContentsResponse> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 29);

/**
 * Counts the number of tokens in the `prompt` sent to a model.
 *
 * Models may tokenize text differently, so each model may return a different
 * `token_count`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.CountTokensRequest
 */
export type CountTokensRequest =
  Message<'google.ai.generativelanguage.v1beta.CountTokensRequest'> & {
    /**
     * Required. The model's resource name. This serves as an ID for the Model to
     * use.
     *
     * This name should match a model name returned by the `ListModels` method.
     *
     * Format: `models/{model}`
     *
     * @generated from field: string model = 1;
     */
    model: string;

    /**
     * Optional. The input given to the model as a prompt. This field is ignored
     * when `generate_content_request` is set.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.Content contents = 2;
     */
    contents: Content[];

    /**
     * Optional. The overall input given to the `Model`. This includes the prompt
     * as well as other model steering information like [system
     * instructions](https://ai.google.dev/gemini-api/docs/system-instructions),
     * and/or function declarations for [function
     * calling](https://ai.google.dev/gemini-api/docs/function-calling).
     * `Model`s/`Content`s and `generate_content_request`s are mutually
     * exclusive. You can either send `Model` + `Content`s or a
     * `generate_content_request`, but never both.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.GenerateContentRequest generate_content_request = 3;
     */
    generateContentRequest?: GenerateContentRequest;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.CountTokensRequest.
 * Use `create(CountTokensRequestSchema)` to create a new message.
 */
export const CountTokensRequestSchema: GenMessage<CountTokensRequest> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 30);

/**
 * A response from `CountTokens`.
 *
 * It returns the model's `token_count` for the `prompt`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.CountTokensResponse
 */
export type CountTokensResponse =
  Message<'google.ai.generativelanguage.v1beta.CountTokensResponse'> & {
    /**
     * The number of tokens that the `Model` tokenizes the `prompt` into. Always
     * non-negative.
     *
     * @generated from field: int32 total_tokens = 1;
     */
    totalTokens: number;

    /**
     * Number of tokens in the cached part of the prompt (the cached content).
     *
     * @generated from field: int32 cached_content_token_count = 5;
     */
    cachedContentTokenCount: number;

    /**
     * Output only. List of modalities that were processed in the request input.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.ModalityTokenCount prompt_tokens_details = 6;
     */
    promptTokensDetails: ModalityTokenCount[];

    /**
     * Output only. List of modalities that were processed in the cached content.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.ModalityTokenCount cache_tokens_details = 7;
     */
    cacheTokensDetails: ModalityTokenCount[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.CountTokensResponse.
 * Use `create(CountTokensResponseSchema)` to create a new message.
 */
export const CountTokensResponseSchema: GenMessage<CountTokensResponse> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 31);

/**
 * Configures the realtime input behavior in `BidiGenerateContent`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.RealtimeInputConfig
 */
export type RealtimeInputConfig =
  Message<'google.ai.generativelanguage.v1beta.RealtimeInputConfig'> & {
    /**
     * Optional. If not set, automatic activity detection is enabled by default.
     * If automatic voice detection is disabled, the client must send activity
     * signals.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.RealtimeInputConfig.AutomaticActivityDetection automatic_activity_detection = 1;
     */
    automaticActivityDetection?: RealtimeInputConfig_AutomaticActivityDetection;

    /**
     * Optional. Defines what effect activity has.
     *
     * @generated from field: optional google.ai.generativelanguage.v1beta.RealtimeInputConfig.ActivityHandling activity_handling = 3;
     */
    activityHandling?: RealtimeInputConfig_ActivityHandling;

    /**
     * Optional. Defines which input is included in the user's turn.
     *
     * @generated from field: optional google.ai.generativelanguage.v1beta.RealtimeInputConfig.TurnCoverage turn_coverage = 4;
     */
    turnCoverage?: RealtimeInputConfig_TurnCoverage;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.RealtimeInputConfig.
 * Use `create(RealtimeInputConfigSchema)` to create a new message.
 */
export const RealtimeInputConfigSchema: GenMessage<RealtimeInputConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 32);

/**
 * Configures automatic detection of activity.
 *
 * @generated from message google.ai.generativelanguage.v1beta.RealtimeInputConfig.AutomaticActivityDetection
 */
export type RealtimeInputConfig_AutomaticActivityDetection =
  Message<'google.ai.generativelanguage.v1beta.RealtimeInputConfig.AutomaticActivityDetection'> & {
    /**
     * Optional. If enabled (the default), detected voice and text input count
     * as activity. If disabled, the client must send activity signals.
     *
     * @generated from field: optional bool disabled = 2;
     */
    disabled?: boolean;

    /**
     * Optional. Determines how likely speech is to be detected.
     *
     * @generated from field: optional google.ai.generativelanguage.v1beta.RealtimeInputConfig.AutomaticActivityDetection.StartSensitivity start_of_speech_sensitivity = 3;
     */
    startOfSpeechSensitivity?: RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity;

    /**
     * Optional. The required duration of detected speech before start-of-speech
     * is committed. The lower this value, the more sensitive the
     * start-of-speech detection is and shorter speech can be recognized.
     * However, this also increases the probability of false positives.
     *
     * @generated from field: optional int32 prefix_padding_ms = 4;
     */
    prefixPaddingMs?: number;

    /**
     * Optional. Determines how likely detected speech is ended.
     *
     * @generated from field: optional google.ai.generativelanguage.v1beta.RealtimeInputConfig.AutomaticActivityDetection.EndSensitivity end_of_speech_sensitivity = 5;
     */
    endOfSpeechSensitivity?: RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity;

    /**
     * Optional. The required duration of detected non-speech (e.g. silence)
     * before end-of-speech is committed. The larger this value, the longer
     * speech gaps can be without interrupting the user's activity but this will
     * increase the model's latency.
     *
     * @generated from field: optional int32 silence_duration_ms = 6;
     */
    silenceDurationMs?: number;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.RealtimeInputConfig.AutomaticActivityDetection.
 * Use `create(RealtimeInputConfig_AutomaticActivityDetectionSchema)` to create a new message.
 */
export const RealtimeInputConfig_AutomaticActivityDetectionSchema: GenMessage<RealtimeInputConfig_AutomaticActivityDetection> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 32, 0);

/**
 * Determines how start of speech is detected.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.RealtimeInputConfig.AutomaticActivityDetection.StartSensitivity
 */
export enum RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity {
  /**
   * The default is START_SENSITIVITY_HIGH.
   *
   * @generated from enum value: START_SENSITIVITY_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * Automatic detection will detect the start of speech more often.
   *
   * @generated from enum value: START_SENSITIVITY_HIGH = 1;
   */
  HIGH = 1,

  /**
   * Automatic detection will detect the start of speech less often.
   *
   * @generated from enum value: START_SENSITIVITY_LOW = 2;
   */
  LOW = 2,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.RealtimeInputConfig.AutomaticActivityDetection.StartSensitivity.
 */
export const RealtimeInputConfig_AutomaticActivityDetection_StartSensitivitySchema: GenEnum<RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 32, 0, 0);

/**
 * Determines how end of speech is detected.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.RealtimeInputConfig.AutomaticActivityDetection.EndSensitivity
 */
export enum RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity {
  /**
   * The default is END_SENSITIVITY_HIGH.
   *
   * @generated from enum value: END_SENSITIVITY_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * Automatic detection ends speech more often.
   *
   * @generated from enum value: END_SENSITIVITY_HIGH = 1;
   */
  HIGH = 1,

  /**
   * Automatic detection ends speech less often.
   *
   * @generated from enum value: END_SENSITIVITY_LOW = 2;
   */
  LOW = 2,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.RealtimeInputConfig.AutomaticActivityDetection.EndSensitivity.
 */
export const RealtimeInputConfig_AutomaticActivityDetection_EndSensitivitySchema: GenEnum<RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 32, 0, 1);

/**
 * The different ways of handling user activity.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.RealtimeInputConfig.ActivityHandling
 */
export enum RealtimeInputConfig_ActivityHandling {
  /**
   * If unspecified, the default behavior is `START_OF_ACTIVITY_INTERRUPTS`.
   *
   * @generated from enum value: ACTIVITY_HANDLING_UNSPECIFIED = 0;
   */
  ACTIVITY_HANDLING_UNSPECIFIED = 0,

  /**
   * If true, start of activity will interrupt the model's response (also
   * called "barge in"). The model's current response will be cut-off in the
   * moment of the interruption. This is the default behavior.
   *
   * @generated from enum value: START_OF_ACTIVITY_INTERRUPTS = 1;
   */
  START_OF_ACTIVITY_INTERRUPTS = 1,

  /**
   * The model's response will not be interrupted.
   *
   * @generated from enum value: NO_INTERRUPTION = 2;
   */
  NO_INTERRUPTION = 2,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.RealtimeInputConfig.ActivityHandling.
 */
export const RealtimeInputConfig_ActivityHandlingSchema: GenEnum<RealtimeInputConfig_ActivityHandling> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 32, 0);

/**
 * Options about which input is included in the user's turn.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.RealtimeInputConfig.TurnCoverage
 */
export enum RealtimeInputConfig_TurnCoverage {
  /**
   * If unspecified, the default behavior is `TURN_INCLUDES_ONLY_ACTIVITY`.
   *
   * @generated from enum value: TURN_COVERAGE_UNSPECIFIED = 0;
   */
  TURN_COVERAGE_UNSPECIFIED = 0,

  /**
   * The users turn only includes activity since the last turn, excluding
   * inactivity (e.g. silence on the audio stream). This is the default
   * behavior.
   *
   * @generated from enum value: TURN_INCLUDES_ONLY_ACTIVITY = 1;
   */
  TURN_INCLUDES_ONLY_ACTIVITY = 1,

  /**
   * The users turn includes all realtime input since the last turn, including
   * inactivity (e.g. silence on the audio stream).
   *
   * @generated from enum value: TURN_INCLUDES_ALL_INPUT = 2;
   */
  TURN_INCLUDES_ALL_INPUT = 2,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.RealtimeInputConfig.TurnCoverage.
 */
export const RealtimeInputConfig_TurnCoverageSchema: GenEnum<RealtimeInputConfig_TurnCoverage> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 32, 1);

/**
 * Session resumption configuration.
 *
 * This message is included in the session configuration as
 * `BidiGenerateContentSetup.session_resumption`. If configured, the server
 * will send `SessionResumptionUpdate` messages.
 *
 * @generated from message google.ai.generativelanguage.v1beta.SessionResumptionConfig
 */
export type SessionResumptionConfig =
  Message<'google.ai.generativelanguage.v1beta.SessionResumptionConfig'> & {
    /**
     * The handle of a previous session. If not present then a new session is
     * created.
     *
     * Session handles come from `SessionResumptionUpdate.token` values in
     * previous connections.
     *
     * @generated from field: optional string handle = 1;
     */
    handle?: string;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.SessionResumptionConfig.
 * Use `create(SessionResumptionConfigSchema)` to create a new message.
 */
export const SessionResumptionConfigSchema: GenMessage<SessionResumptionConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 33);

/**
 * Enables context window compression  a mechanism for managing the model's
 * context window so that it does not exceed a given length.
 *
 * @generated from message google.ai.generativelanguage.v1beta.ContextWindowCompressionConfig
 */
export type ContextWindowCompressionConfig =
  Message<'google.ai.generativelanguage.v1beta.ContextWindowCompressionConfig'> & {
    /**
     * The context window compression mechanism used.
     *
     * @generated from oneof google.ai.generativelanguage.v1beta.ContextWindowCompressionConfig.compression_mechanism
     */
    compressionMechanism:
      | {
          /**
           * A sliding-window mechanism.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.ContextWindowCompressionConfig.SlidingWindow sliding_window = 2;
           */
          value: ContextWindowCompressionConfig_SlidingWindow;
          case: 'slidingWindow';
        }
      | { case: undefined; value?: undefined };

    /**
     * The number of tokens (before running a turn) required to trigger a context
     * window compression.
     *
     * This can be used to balance quality against latency as shorter context
     * windows may result in faster model responses. However, any compression
     * operation will cause a temporary latency increase, so they should not be
     * triggered frequently.
     *
     * If not set, the default is 80% of the model's context window limit. This
     * leaves 20% for the next user request/model response.
     *
     * @generated from field: optional int64 trigger_tokens = 1;
     */
    triggerTokens?: bigint;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.ContextWindowCompressionConfig.
 * Use `create(ContextWindowCompressionConfigSchema)` to create a new message.
 */
export const ContextWindowCompressionConfigSchema: GenMessage<ContextWindowCompressionConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 34);

/**
 * The SlidingWindow method operates by discarding content at the beginning of
 * the context window. The resulting context will always begin at the start of
 * a USER role turn. System instructions and any
 * `BidiGenerateContentSetup.prefix_turns` will always remain at the beginning
 * of the result.
 *
 * @generated from message google.ai.generativelanguage.v1beta.ContextWindowCompressionConfig.SlidingWindow
 */
export type ContextWindowCompressionConfig_SlidingWindow =
  Message<'google.ai.generativelanguage.v1beta.ContextWindowCompressionConfig.SlidingWindow'> & {
    /**
     * The target number of tokens to keep. The default value is
     * trigger_tokens/2.
     *
     * Discarding parts of the context window causes a temporary latency
     * increase so this value should be calibrated to avoid frequent compression
     * operations.
     *
     * @generated from field: optional int64 target_tokens = 1;
     */
    targetTokens?: bigint;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.ContextWindowCompressionConfig.SlidingWindow.
 * Use `create(ContextWindowCompressionConfig_SlidingWindowSchema)` to create a new message.
 */
export const ContextWindowCompressionConfig_SlidingWindowSchema: GenMessage<ContextWindowCompressionConfig_SlidingWindow> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 34, 0);

/**
 * The audio transcription configuration.
 *
 * @generated from message google.ai.generativelanguage.v1beta.AudioTranscriptionConfig
 */
export type AudioTranscriptionConfig =
  Message<'google.ai.generativelanguage.v1beta.AudioTranscriptionConfig'> & {};

/**
 * Describes the message google.ai.generativelanguage.v1beta.AudioTranscriptionConfig.
 * Use `create(AudioTranscriptionConfigSchema)` to create a new message.
 */
export const AudioTranscriptionConfigSchema: GenMessage<AudioTranscriptionConfig> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 35);

/**
 * Message to be sent in the first (and only in the first)
 * `BidiGenerateContentClientMessage`. Contains configuration that will apply
 * for the duration of the streaming RPC.
 *
 * Clients should wait for a `BidiGenerateContentSetupComplete` message before
 * sending any additional messages.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentSetup
 */
export type BidiGenerateContentSetup =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentSetup'> & {
    /**
     * Required. The model's resource name. This serves as an ID for the Model to
     * use.
     *
     * Format: `models/{model}`
     *
     * @generated from field: string model = 1;
     */
    model: string;

    /**
     * Optional. Generation config.
     *
     * The following fields are not supported:
     *
     *  - `response_logprobs`
     *  - `response_mime_type`
     *  - `logprobs`
     *  - `response_schema`
     *  - `response_json_schema`
     *  - `stop_sequence`
     *  - `routing_config`
     *  - `audio_timestamp`
     *
     * @generated from field: google.ai.generativelanguage.v1beta.GenerationConfig generation_config = 2;
     */
    generationConfig?: GenerationConfig;

    /**
     * Optional. The user provided system instructions for the model.
     *
     * Note: Only text should be used in parts and content in each part will be
     * in a separate paragraph.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.Content system_instruction = 3;
     */
    systemInstruction?: Content;

    /**
     * Optional. A list of `Tools` the model may use to generate the next
     * response.
     *
     * A `Tool` is a piece of code that enables the system to interact with
     * external systems to perform an action, or set of actions, outside of
     * knowledge and scope of the model.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.Tool tools = 4;
     */
    tools: Tool[];

    /**
     * Optional. Configures the handling of realtime input.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.RealtimeInputConfig realtime_input_config = 6;
     */
    realtimeInputConfig?: RealtimeInputConfig;

    /**
     * Optional. Configures session resumption mechanism.
     *
     * If included, the server will send `SessionResumptionUpdate` messages.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.SessionResumptionConfig session_resumption = 7;
     */
    sessionResumption?: SessionResumptionConfig;

    /**
     * Optional. Configures a context window compression mechanism.
     *
     * If included, the server will automatically reduce the size of the context
     * when it exceeds the configured length.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.ContextWindowCompressionConfig context_window_compression = 8;
     */
    contextWindowCompression?: ContextWindowCompressionConfig;

    /**
     * Optional. If set, enables transcription of voice input. The transcription
     * aligns with the input audio language, if configured.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.AudioTranscriptionConfig input_audio_transcription = 10;
     */
    inputAudioTranscription?: AudioTranscriptionConfig;

    /**
     * Optional. If set, enables transcription of the model's audio output. The
     * transcription aligns with the language code specified for the output
     * audio, if configured.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.AudioTranscriptionConfig output_audio_transcription = 11;
     */
    outputAudioTranscription?: AudioTranscriptionConfig;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentSetup.
 * Use `create(BidiGenerateContentSetupSchema)` to create a new message.
 */
export const BidiGenerateContentSetupSchema: GenMessage<BidiGenerateContentSetup> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 36);

/**
 * Incremental update of the current conversation delivered from the client.
 * All of the content here is unconditionally appended to the conversation
 * history and used as part of the prompt to the model to generate content.
 *
 * A message here will interrupt any current model generation.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentClientContent
 */
export type BidiGenerateContentClientContent =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentClientContent'> & {
    /**
     * Optional. The content appended to the current conversation with the model.
     *
     * For single-turn queries, this is a single instance. For multi-turn
     * queries, this is a repeated field that contains conversation history and
     * the latest request.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.Content turns = 1;
     */
    turns: Content[];

    /**
     * Optional. If true, indicates that the server content generation should
     * start with the currently accumulated prompt. Otherwise, the server awaits
     * additional messages before starting generation.
     *
     * @generated from field: bool turn_complete = 2;
     */
    turnComplete: boolean;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentClientContent.
 * Use `create(BidiGenerateContentClientContentSchema)` to create a new message.
 */
export const BidiGenerateContentClientContentSchema: GenMessage<BidiGenerateContentClientContent> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 37);

/**
 * User input that is sent in real time.
 *
 * The different modalities (audio, video and text) are handled as concurrent
 * streams. The ordering across these streams is not guaranteed.
 *
 * This is different from
 * [BidiGenerateContentClientContent][google.ai.generativelanguage.v1beta.BidiGenerateContentClientContent]
 * in a few ways:
 *
 * * Can be sent continuously without interruption to model generation.
 * * If there is a need to mix data interleaved across the
 *   [BidiGenerateContentClientContent][google.ai.generativelanguage.v1beta.BidiGenerateContentClientContent]
 *   and the
 *   [BidiGenerateContentRealtimeInput][google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput],
 *   the server attempts to optimize for best response, but there are no
 *   guarantees.
 * * End of turn is not explicitly specified, but is rather derived from user
 *   activity (for example, end of speech).
 * * Even before the end of turn, the data is processed incrementally
 *   to optimize for a fast start of the response from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput
 */
export type BidiGenerateContentRealtimeInput =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput'> & {
    /**
     * Optional. Inlined bytes data for media input. Multiple `media_chunks` are
     * not supported, all but the first will be ignored.
     *
     * DEPRECATED: Use one of `audio`, `video`, or `text` instead.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.Blob media_chunks = 1;
     */
    mediaChunks: Blob[];

    /**
     * Optional. These form the realtime audio input stream.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.Blob audio = 2;
     */
    audio?: Blob;

    /**
     * Optional. Indicates that the audio stream has ended, e.g. because the
     * microphone was turned off.
     *
     * This should only be sent when automatic activity detection is enabled
     * (which is the default).
     *
     * The client can reopen the stream by sending an audio message.
     *
     * @generated from field: optional bool audio_stream_end = 3;
     */
    audioStreamEnd?: boolean;

    /**
     * Optional. These form the realtime video input stream.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.Blob video = 4;
     */
    video?: Blob;

    /**
     * Optional. These form the realtime text input stream.
     *
     * @generated from field: optional string text = 5;
     */
    text?: string;

    /**
     * Optional. Marks the start of user activity. This can only be sent if
     * automatic (i.e. server-side) activity detection is disabled.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput.ActivityStart activity_start = 6;
     */
    activityStart?: BidiGenerateContentRealtimeInput_ActivityStart;

    /**
     * Optional. Marks the end of user activity. This can only be sent if
     * automatic (i.e. server-side) activity detection is disabled.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput.ActivityEnd activity_end = 7;
     */
    activityEnd?: BidiGenerateContentRealtimeInput_ActivityEnd;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput.
 * Use `create(BidiGenerateContentRealtimeInputSchema)` to create a new message.
 */
export const BidiGenerateContentRealtimeInputSchema: GenMessage<BidiGenerateContentRealtimeInput> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 38);

/**
 * Marks the start of user activity.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput.ActivityStart
 */
export type BidiGenerateContentRealtimeInput_ActivityStart =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput.ActivityStart'> & {};

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput.ActivityStart.
 * Use `create(BidiGenerateContentRealtimeInput_ActivityStartSchema)` to create a new message.
 */
export const BidiGenerateContentRealtimeInput_ActivityStartSchema: GenMessage<BidiGenerateContentRealtimeInput_ActivityStart> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 38, 0);

/**
 * Marks the end of user activity.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput.ActivityEnd
 */
export type BidiGenerateContentRealtimeInput_ActivityEnd =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput.ActivityEnd'> & {};

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput.ActivityEnd.
 * Use `create(BidiGenerateContentRealtimeInput_ActivityEndSchema)` to create a new message.
 */
export const BidiGenerateContentRealtimeInput_ActivityEndSchema: GenMessage<BidiGenerateContentRealtimeInput_ActivityEnd> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 38, 1);

/**
 * Client generated response to a `ToolCall` received from the server.
 * Individual `FunctionResponse` objects are matched to the respective
 * `FunctionCall` objects by the `id` field.
 *
 * Note that in the unary and server-streaming GenerateContent APIs function
 * calling happens by exchanging the `Content` parts, while in the bidi
 * GenerateContent APIs function calling happens over these dedicated set of
 * messages.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentToolResponse
 */
export type BidiGenerateContentToolResponse =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentToolResponse'> & {
    /**
     * Optional. The response to the function calls.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.FunctionResponse function_responses = 1;
     */
    functionResponses: FunctionResponse[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentToolResponse.
 * Use `create(BidiGenerateContentToolResponseSchema)` to create a new message.
 */
export const BidiGenerateContentToolResponseSchema: GenMessage<BidiGenerateContentToolResponse> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 39);

/**
 * Messages sent by the client in the BidiGenerateContent call.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentClientMessage
 */
export type BidiGenerateContentClientMessage =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentClientMessage'> & {
    /**
     * The type of the message.
     *
     * @generated from oneof google.ai.generativelanguage.v1beta.BidiGenerateContentClientMessage.message_type
     */
    messageType:
      | {
          /**
           * Optional. Session configuration sent only in the first client message.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentSetup setup = 1;
           */
          value: BidiGenerateContentSetup;
          case: 'setup';
        }
      | {
          /**
           * Optional. Incremental update of the current conversation delivered from
           * the client.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentClientContent client_content = 2;
           */
          value: BidiGenerateContentClientContent;
          case: 'clientContent';
        }
      | {
          /**
           * Optional. User input that is sent in real time.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput realtime_input = 3;
           */
          value: BidiGenerateContentRealtimeInput;
          case: 'realtimeInput';
        }
      | {
          /**
           * Optional. Response to a `ToolCallMessage` received from the server.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentToolResponse tool_response = 4;
           */
          value: BidiGenerateContentToolResponse;
          case: 'toolResponse';
        }
      | { case: undefined; value?: undefined };
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentClientMessage.
 * Use `create(BidiGenerateContentClientMessageSchema)` to create a new message.
 */
export const BidiGenerateContentClientMessageSchema: GenMessage<BidiGenerateContentClientMessage> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 40);

/**
 * Sent in response to a `BidiGenerateContentSetup` message from the client.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentSetupComplete
 */
export type BidiGenerateContentSetupComplete =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentSetupComplete'> & {};

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentSetupComplete.
 * Use `create(BidiGenerateContentSetupCompleteSchema)` to create a new message.
 */
export const BidiGenerateContentSetupCompleteSchema: GenMessage<BidiGenerateContentSetupComplete> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 41);

/**
 * Incremental server update generated by the model in response to client
 * messages.
 *
 * Content is generated as quickly as possible, and not in real time. Clients
 * may choose to buffer and play it out in real time.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentServerContent
 */
export type BidiGenerateContentServerContent =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentServerContent'> & {
    /**
     * Output only. The content that the model has generated as part of the
     * current conversation with the user.
     *
     * @generated from field: optional google.ai.generativelanguage.v1beta.Content model_turn = 1;
     */
    modelTurn?: Content;

    /**
     * Output only. If true, indicates that the model is done generating.
     *
     * When model is interrupted while generating there will be no
     * 'generation_complete' message in interrupted turn, it will go through
     * 'interrupted > turn_complete'.
     *
     * When model assumes realtime playback there will be delay between
     * generation_complete and turn_complete that is caused by model waiting for
     * playback to finish.
     *
     * @generated from field: bool generation_complete = 5;
     */
    generationComplete: boolean;

    /**
     * Output only. If true, indicates that the model has completed its turn.
     * Generation will only start in response to additional client messages.
     *
     * @generated from field: bool turn_complete = 2;
     */
    turnComplete: boolean;

    /**
     * Output only. If true, indicates that a client message has interrupted
     * current model generation. If the client is playing out the content in real
     * time, this is a good signal to stop and empty the current playback queue.
     *
     * @generated from field: bool interrupted = 3;
     */
    interrupted: boolean;

    /**
     * Output only. Grounding metadata for the generated content.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.GroundingMetadata grounding_metadata = 4;
     */
    groundingMetadata?: GroundingMetadata;

    /**
     * Output only. Input audio transcription. The transcription is sent
     * independently of the other server messages and there is no guaranteed
     * ordering.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentTranscription input_transcription = 6;
     */
    inputTranscription?: BidiGenerateContentTranscription;

    /**
     * Output only. Output audio transcription. These transcriptions are part of
     * the Generation output of the server. The last output transcription of this
     * turn is sent before either `generation_complete` or `interrupted`, which in
     * turn are followed by `turn_complete`. There is no guaranteed exact ordering
     * between transcriptions and other `model_turn` output but the server tries
     * to send the transcripts close to the corresponding audio output.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentTranscription output_transcription = 7;
     */
    outputTranscription?: BidiGenerateContentTranscription;

    /**
     * @generated from field: google.ai.generativelanguage.v1beta.UrlContextMetadata url_context_metadata = 9;
     */
    urlContextMetadata?: UrlContextMetadata;

    /**
     * Output only. If true, indicates that the model is not generating content
     * because it is waiting for more input from the user, e.g. because it expects
     * the user to continue talking.
     *
     * @generated from field: bool waiting_for_input = 10;
     */
    waitingForInput: boolean;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentServerContent.
 * Use `create(BidiGenerateContentServerContentSchema)` to create a new message.
 */
export const BidiGenerateContentServerContentSchema: GenMessage<BidiGenerateContentServerContent> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 42);

/**
 * Request for the client to execute the `function_calls` and return the
 * responses with the matching `id`s.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentToolCall
 */
export type BidiGenerateContentToolCall =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentToolCall'> & {
    /**
     * Output only. The function call to be executed.
     *
     * @generated from field: repeated google.ai.generativelanguage.v1beta.FunctionCall function_calls = 2;
     */
    functionCalls: FunctionCall[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentToolCall.
 * Use `create(BidiGenerateContentToolCallSchema)` to create a new message.
 */
export const BidiGenerateContentToolCallSchema: GenMessage<BidiGenerateContentToolCall> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 43);

/**
 * Notification for the client that a previously issued `ToolCallMessage`
 * with the specified `id`s should not have been executed and should be
 * cancelled. If there were side-effects to those tool calls, clients may
 * attempt to undo the tool calls. This message occurs only in cases where the
 * clients interrupt server turns.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentToolCallCancellation
 */
export type BidiGenerateContentToolCallCancellation =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentToolCallCancellation'> & {
    /**
     * Output only. The ids of the tool calls to be cancelled.
     *
     * @generated from field: repeated string ids = 1;
     */
    ids: string[];
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentToolCallCancellation.
 * Use `create(BidiGenerateContentToolCallCancellationSchema)` to create a new message.
 */
export const BidiGenerateContentToolCallCancellationSchema: GenMessage<BidiGenerateContentToolCallCancellation> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 44);

/**
 * A notice that the server will soon disconnect.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GoAway
 */
export type GoAway = Message<'google.ai.generativelanguage.v1beta.GoAway'> & {
  /**
   * The remaining time before the connection will be terminated as ABORTED.
   *
   * This duration will never be less than a model-specific minimum, which will
   * be specified together with the rate limits for the model.
   *
   * @generated from field: google.protobuf.Duration time_left = 1;
   */
  timeLeft?: Duration;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GoAway.
 * Use `create(GoAwaySchema)` to create a new message.
 */
export const GoAwaySchema: GenMessage<GoAway> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 45);

/**
 * Update of the session resumption state.
 *
 * Only sent if `BidiGenerateContentSetup.session_resumption` was set.
 *
 * @generated from message google.ai.generativelanguage.v1beta.SessionResumptionUpdate
 */
export type SessionResumptionUpdate =
  Message<'google.ai.generativelanguage.v1beta.SessionResumptionUpdate'> & {
    /**
     * New handle that represents a state that can be resumed. Empty if
     * `resumable`=false.
     *
     * @generated from field: string new_handle = 1;
     */
    newHandle: string;

    /**
     * True if the current session can be resumed at this point.
     *
     * Resumption is not possible at some points in the session. For example, when
     * the model is executing function calls or generating. Resuming the session
     * (using a previous session token) in such a state will result in some data
     * loss. In these cases, `new_handle` will be empty and `resumable` will be
     * false.
     *
     * @generated from field: bool resumable = 2;
     */
    resumable: boolean;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.SessionResumptionUpdate.
 * Use `create(SessionResumptionUpdateSchema)` to create a new message.
 */
export const SessionResumptionUpdateSchema: GenMessage<SessionResumptionUpdate> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 46);

/**
 * Transcription of audio (input or output).
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentTranscription
 */
export type BidiGenerateContentTranscription =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentTranscription'> & {
    /**
     * Transcription text.
     *
     * @generated from field: string text = 1;
     */
    text: string;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentTranscription.
 * Use `create(BidiGenerateContentTranscriptionSchema)` to create a new message.
 */
export const BidiGenerateContentTranscriptionSchema: GenMessage<BidiGenerateContentTranscription> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 47);

/**
 * Response message for the BidiGenerateContent call.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BidiGenerateContentServerMessage
 */
export type BidiGenerateContentServerMessage =
  Message<'google.ai.generativelanguage.v1beta.BidiGenerateContentServerMessage'> & {
    /**
     * The type of the message.
     *
     * @generated from oneof google.ai.generativelanguage.v1beta.BidiGenerateContentServerMessage.message_type
     */
    messageType:
      | {
          /**
           * Output only. Sent in response to a `BidiGenerateContentSetup` message
           * from the client when setup is complete.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentSetupComplete setup_complete = 2;
           */
          value: BidiGenerateContentSetupComplete;
          case: 'setupComplete';
        }
      | {
          /**
           * Output only. Content generated by the model in response to client
           * messages.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentServerContent server_content = 3;
           */
          value: BidiGenerateContentServerContent;
          case: 'serverContent';
        }
      | {
          /**
           * Output only. Request for the client to execute the `function_calls` and
           * return the responses with the matching `id`s.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentToolCall tool_call = 4;
           */
          value: BidiGenerateContentToolCall;
          case: 'toolCall';
        }
      | {
          /**
           * Output only. Notification for the client that a previously issued
           * `ToolCallMessage` with the specified `id`s should be cancelled.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.BidiGenerateContentToolCallCancellation tool_call_cancellation = 5;
           */
          value: BidiGenerateContentToolCallCancellation;
          case: 'toolCallCancellation';
        }
      | {
          /**
           * Output only. A notice that the server will soon disconnect.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.GoAway go_away = 6;
           */
          value: GoAway;
          case: 'goAway';
        }
      | {
          /**
           * Output only. Update of the session resumption state.
           *
           * @generated from field: google.ai.generativelanguage.v1beta.SessionResumptionUpdate session_resumption_update = 7;
           */
          value: SessionResumptionUpdate;
          case: 'sessionResumptionUpdate';
        }
      | { case: undefined; value?: undefined };

    /**
     * Output only. Usage metadata about the response(s).
     *
     * @generated from field: google.ai.generativelanguage.v1beta.UsageMetadata usage_metadata = 10;
     */
    usageMetadata?: UsageMetadata;
  };

/**
 * Describes the message google.ai.generativelanguage.v1beta.BidiGenerateContentServerMessage.
 * Use `create(BidiGenerateContentServerMessageSchema)` to create a new message.
 */
export const BidiGenerateContentServerMessageSchema: GenMessage<BidiGenerateContentServerMessage> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 48);

/**
 * Usage metadata about response(s).
 *
 * @generated from message google.ai.generativelanguage.v1beta.UsageMetadata
 */
export type UsageMetadata = Message<'google.ai.generativelanguage.v1beta.UsageMetadata'> & {
  /**
   * Output only. Number of tokens in the prompt. When `cached_content` is set,
   * this is still the total effective prompt size meaning this includes the
   * number of tokens in the cached content.
   *
   * @generated from field: int32 prompt_token_count = 1;
   */
  promptTokenCount: number;

  /**
   * Number of tokens in the cached part of the prompt (the cached content)
   *
   * @generated from field: int32 cached_content_token_count = 4;
   */
  cachedContentTokenCount: number;

  /**
   * Output only. Total number of tokens across all the generated response
   * candidates.
   *
   * @generated from field: int32 response_token_count = 2;
   */
  responseTokenCount: number;

  /**
   * Output only. Number of tokens present in tool-use prompt(s).
   *
   * @generated from field: int32 tool_use_prompt_token_count = 8;
   */
  toolUsePromptTokenCount: number;

  /**
   * Output only. Number of tokens of thoughts for thinking models.
   *
   * @generated from field: int32 thoughts_token_count = 10;
   */
  thoughtsTokenCount: number;

  /**
   * Output only. Total token count for the generation request (prompt +
   * response candidates).
   *
   * @generated from field: int32 total_token_count = 3;
   */
  totalTokenCount: number;

  /**
   * Output only. List of modalities that were processed in the request input.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.ModalityTokenCount prompt_tokens_details = 5;
   */
  promptTokensDetails: ModalityTokenCount[];

  /**
   * Output only. List of modalities of the cached content in the request input.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.ModalityTokenCount cache_tokens_details = 6;
   */
  cacheTokensDetails: ModalityTokenCount[];

  /**
   * Output only. List of modalities that were returned in the response.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.ModalityTokenCount response_tokens_details = 7;
   */
  responseTokensDetails: ModalityTokenCount[];

  /**
   * Output only. List of modalities that were processed for tool-use request
   * inputs.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.ModalityTokenCount tool_use_prompt_tokens_details = 9;
   */
  toolUsePromptTokensDetails: ModalityTokenCount[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.UsageMetadata.
 * Use `create(UsageMetadataSchema)` to create a new message.
 */
export const UsageMetadataSchema: GenMessage<UsageMetadata> =
  /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 49);

/**
 * Type of task for which the embedding will be used.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.TaskType
 */
export enum TaskType {
  /**
   * Unset value, which will default to one of the other enum values.
   *
   * @generated from enum value: TASK_TYPE_UNSPECIFIED = 0;
   */
  TASK_TYPE_UNSPECIFIED = 0,

  /**
   * Specifies the given text is a query in a search/retrieval setting.
   *
   * @generated from enum value: RETRIEVAL_QUERY = 1;
   */
  RETRIEVAL_QUERY = 1,

  /**
   * Specifies the given text is a document from the corpus being searched.
   *
   * @generated from enum value: RETRIEVAL_DOCUMENT = 2;
   */
  RETRIEVAL_DOCUMENT = 2,

  /**
   * Specifies the given text will be used for STS.
   *
   * @generated from enum value: SEMANTIC_SIMILARITY = 3;
   */
  SEMANTIC_SIMILARITY = 3,

  /**
   * Specifies that the given text will be classified.
   *
   * @generated from enum value: CLASSIFICATION = 4;
   */
  CLASSIFICATION = 4,

  /**
   * Specifies that the embeddings will be used for clustering.
   *
   * @generated from enum value: CLUSTERING = 5;
   */
  CLUSTERING = 5,

  /**
   * Specifies that the given text will be used for question answering.
   *
   * @generated from enum value: QUESTION_ANSWERING = 6;
   */
  QUESTION_ANSWERING = 6,

  /**
   * Specifies that the given text will be used for fact verification.
   *
   * @generated from enum value: FACT_VERIFICATION = 7;
   */
  FACT_VERIFICATION = 7,

  /**
   * Specifies that the given text will be used for code retrieval.
   *
   * @generated from enum value: CODE_RETRIEVAL_QUERY = 8;
   */
  CODE_RETRIEVAL_QUERY = 8,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.TaskType.
 */
export const TaskTypeSchema: GenEnum<TaskType> =
  /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 0);

/**
 * API for using Large Models that generate multimodal content and have
 * additional capabilities beyond text generation.
 *
 * @generated from service google.ai.generativelanguage.v1beta.GenerativeService
 */
export const GenerativeService: GenService<{
  /**
   * Generates a model response given an input `GenerateContentRequest`.
   * Refer to the [text generation
   * guide](https://ai.google.dev/gemini-api/docs/text-generation) for detailed
   * usage information. Input capabilities differ between models, including
   * tuned models. Refer to the [model
   * guide](https://ai.google.dev/gemini-api/docs/models/gemini) and [tuning
   * guide](https://ai.google.dev/gemini-api/docs/model-tuning) for details.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.GenerateContent
   */
  generateContent: {
    methodKind: 'unary';
    input: typeof GenerateContentRequestSchema;
    output: typeof GenerateContentResponseSchema;
  };
  /**
   * Generates a grounded answer from the model given an input
   * `GenerateAnswerRequest`.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.GenerateAnswer
   */
  generateAnswer: {
    methodKind: 'unary';
    input: typeof GenerateAnswerRequestSchema;
    output: typeof GenerateAnswerResponseSchema;
  };
  /**
   * Generates a [streamed
   * response](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#generate-a-text-stream)
   * from the model given an input `GenerateContentRequest`.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.StreamGenerateContent
   */
  streamGenerateContent: {
    methodKind: 'server_streaming';
    input: typeof GenerateContentRequestSchema;
    output: typeof GenerateContentResponseSchema;
  };
  /**
   * Generates a text embedding vector from the input `Content` using the
   * specified [Gemini Embedding
   * model](https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding).
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.EmbedContent
   */
  embedContent: {
    methodKind: 'unary';
    input: typeof EmbedContentRequestSchema;
    output: typeof EmbedContentResponseSchema;
  };
  /**
   * Generates multiple embedding vectors from the input `Content` which
   * consists of a batch of strings represented as `EmbedContentRequest`
   * objects.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents
   */
  batchEmbedContents: {
    methodKind: 'unary';
    input: typeof BatchEmbedContentsRequestSchema;
    output: typeof BatchEmbedContentsResponseSchema;
  };
  /**
   * Runs a model's tokenizer on input `Content` and returns the token count.
   * Refer to the [tokens guide](https://ai.google.dev/gemini-api/docs/tokens)
   * to learn more about tokens.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.CountTokens
   */
  countTokens: {
    methodKind: 'unary';
    input: typeof CountTokensRequestSchema;
    output: typeof CountTokensResponseSchema;
  };
  /**
   * Low-Latency bidirectional streaming API that supports audio and video
   * streaming inputs can produce multimodal output streams (audio and text).
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent
   */
  bidiGenerateContent: {
    methodKind: 'bidi_streaming';
    input: typeof BidiGenerateContentClientMessageSchema;
    output: typeof BidiGenerateContentServerMessageSchema;
  };
}> = /*@__PURE__*/ serviceDesc(file_google_ai_generativelanguage_v1beta_generative_service, 0);
